POSIX , SUS, XSI

Portable Operating System Interface
POSIX是给Unix/Linux系统使用的通用调用接口(SCI, System Call Interface)，以期实现同一个程序不需要任何修改就可以实现在不同的Unix/Linux系统间的移植，windows 后来也做了一些工作来支持POSIX，比如windows NT 。POSIX.1 包含了ISO C的标准函数库两部分(C语言就是为了写Unix开发的语言），分为必须部分和可选部分，只有将这两部分全部实现的才能被称作Unix操作系统
ISO C VS POSIX C

    ISO C标准
    1989年， C程序设计语言的ANSI标准得到批准，此标准被采纳为国际标准ISO/IEC 9899
    1990年， 1990 ISO C标准的意图是提供C程序的可移植性，使其能够适合于大量不同的操作系统
    1991年， ISO C标准被更新，并批准为iso/iec 9899:1999 , ISO C标准定义的头文件有24个
    IEEE POSIX 标准
    POSIX的目的是提升应用程序在各种UNIX系统环境之间的可移植性
    POSIX.1包含了ISO C标准函数库，同时包含两部分：必须部分和可选部分（X/Open系统接口(X/Open System Interface,XSI)）。POSIX.1中的X/Open系统选项描述了可选的接口,只有遵循XSI实现的才能称为UNIX系统。

Single UNIX Specification(SUS)

SUS是POSIX.1标准的一个超集，它定义了一些附加结构扩展了POSIX.1规范提供的功能POSIX.1相当于是SUS 中基本规范部分。
总结: ISO C是POSIX的子集，POSIX是SUS的子集，SUS是POSIX的扩展，X/Open系统接口(XSI)是POSIX.1中的接口，描述了POSIX中的可选接口，只有遵循XSI的实现才能称为UNIX系统
ISO/IEC 9899:1990 (C89, C90).
The 15 standard headers in C89 are:

<assert.h>  <ctype.h>   <errno.h>   <float.h>   <limits.h>  <locale.h>
<math.h>    <setjmp.h>  <signal.h>  <stdarg.h>  <stddef.h>  <stdio.h>
<stdlib.h>  <string.h>  <time.h>

ISO/IEC 9899:1990/Amd.1:1995.
The 3 extra headers introduced in C94* (Amendment 1) are:

<iso646.h>  <wchar.h>   <wctype.h>

ISO/IEC 9899:1999 (C99).
The 6 extra headers in C99 are:

<complex.h>     <fenv.h>            <inttypes.h>        <stdbool.h>     <stdint.h>      <tgmath.h>

ISO/IEC 9899:2011 (C11). The 5 extra headers in C2011 (for a total of 29) are:

<stdalign.h>        <stdatomic.h>       <stdnoreturn.h> <threads.h>     <uchar.h>

ISO/IEC 9045:2008 (POSIX 2008, SUS)
POSIX.1 C99

…

POSIX.1 STANDARD

<arpa/inet.h>   <dirent.h>      <fcntl.h>       <fnmatch.h>     <glob.h>        <grp.h>
<net/if.h>      <netdb.h>       <netinet/in.h>  <netinet/tcp.h> <pwd.h>         <regex.h>
<sys/mman.h>    <sys/select.h>  <sys/socket.h>  <sys/stat.h>    <sys/time.h>    <sys/times.h>
<sys/types.h>   <sys/un.h>      <sys/utsname.h> <sys/wait.h>    <tar.h>         <termios.h>
<unistd.h>      <utime.h>       <wordexp.h>

POSIX.1 XSI

<cpio.h>        <dlfcn.h>       <fmtmsg.h>      <ftw.h>         <iconv.h>       <langinfo.h>
<libgen.h>      <monetary.h>    <ndbm.h>        <nl_types.h>    <poll.h>        <search.h>
<strings.h>     <syslog.h>      <sys/ipc.h>     <sys/msg.h>     <sys/resource.h><sys/sem.h>
<sys/shm.h>     <sys/statvfs.h> <sys/time.h>    <sys/ timeb.h>  <sys/uio.h>     <ucontext.h>
<ulimit.h>      <utmpx.h>   

POSIX.1 Asynchronous I/O:

<aio.h>         <mqueue.h>      <pthread.h>     <sched.h>       <semaphore.h>   <spawn.h>
<stropts.h>     <trace.h>

On some platforms, X/Open Curses requires another set of headers:

<varargs.h>     <curses.h>      <term.h>        <uncntrl.h>

Q:遵循POSIX标准的OS是否具有完全相同系统调用函数原型???
A:支持POSIX标准的OS都提供一套符合POSIX标准的接口规范，即SUS(ANSI C+ POSIX.1+…)而不是相同的SCI

POSIX(printf vprintf asprintf){

1. print系列函数根据format 参数生成输出内容。
2. printf和vprintf函数把输出内容写到stdout，即标准输出流；
3. fprintf和vfprintf函数把输出内容写到给定的stream流；
4. sprintf、snprintf、 vsprintf和vsnprintf函数把输出内容存放到字符串str中。

返回值说明：返回值为格式化字符串长度：不包括结尾字符'\0'。 snprintf() and vsnprintf()
中size值为格式化字符串最大长度减1：包括接口字符'\0'

int asprintf(char **strp, const char *fmt, ...);
int vasprintf(char **strp, const char *fmt, va_list ap);
返回值说明：返回值为格式化字符串长度：类似sprintf，如果内存申请失败或其他类型的错误，返回为-1；且strp为未定义。

log.sh 有对stdarg.h的详细描述；

}

POSIX(){

    设计网络服务器其实跟设计操作系统类似， 就是尽量让资源等待不占用CPU， 让CPU去干重要的活， 没活或者没数据
的就空闲不管它， 说白了就是业务逻辑处理时间不能跟网络资源等待串行， 有了这个原则， 去设计东西就可以轻松很多，
有兴趣的可以看看我开发的通信库 http://sbase.googlecode.com 

    关于服务器开发，必须要做到网络与逻辑分开。逻辑处理线程池可以根据各自需求整合到网络引擎中，或者整合到逻辑中。
    关于ACE或者各种开源引擎，我觉得好比是一件漂亮的衣服，但是要用的话很难控制甚至达不到开发者的初衷，
发生问题的时候你只能等待新版本的更新了，这方面我以前可是吃尽了苦头。
}

# http://blog.csdn.net/solstice/article/details/7720755
POSIX(用条件变量实现事件等待器的正确与错误做法){

# http://blog.csdn.net/solstice/article/details/11432817
我在拙作《Linux 多线程服务端编程：使用 muduo C++ 网络库》第 2.2 节总结了条件变量的使用要点：

条件变量只有一种正确使用的方式，几乎不可能用错。对于 wait 端：
1. 必须与 mutex 一起使用，该布尔表达式的读写需受此 mutex 保护。
2. 在 mutex 已上锁的时候才能调用 wait()。
3. 把判断布尔条件和 wait() 放到 while 循环中。

对于 signal/broadcast 端：
1. 不一定要在 mutex 已上锁的情况下调用 signal （理论上）。
2. 在 signal 之前一般要修改布尔表达式。
3. 修改布尔表达式通常要用 mutex 保护（至少用作 full memory barrier）。
4. 注意区分 signal 与 broadcast：“broadcast 通常用于表明状态变化，signal 通常用于表示资源可用。
（broadcast should generally be used to indicate state change rather than resource availability。）”

版本四五六：正确。仅限 single waiter 使用。
版本七：最佳。可供 multiple waiters 使用。
}
POSIX(多线程服务器的适用场合){
1. 服务器开发
    跑在多核机器上的 Linux 用户态的没有用户界面的长期运行的网络应用程序。“长期运行”的意思不是指程序 7x24 
不重启，而是程序不会因为无事可做而退出，它会等着下一个请求的到来。例如 wget 不是长期运行的，httpd 是
长期运行的。
---------------------------------------
2. 进程 VS 线程
“进程”指的是 fork() 系统调用的产物。
“线程”指的是 pthread_create() 的产物，而且我指的 pthreads 是 NPTL 的，每个线程由 clone() 产生，
对应一个内核的 task_struct。本文所用的开发语言是 C++，运行环境为 Linux。

一台机器，一台拥有至少 4 个核的普通服务器。如果要在一台多核机器上提供一种服务或执行一个任务，可用的模式有：
运行一个单线程的进程
运行一个多线程的进程
运行多个单线程的进程
运行多个多线程的进程

这些模式之间的比较已经是老生常谈，简单地总结：
    模式 1 是不可伸缩的 (scalable)，不能发挥多核机器的计算能力；
    模式 3 是目前公认的主流模式。它有两种子模式：
        3a 简单地把模式 1 中的进程运行多份，如果能用多个 tcp port 对外提供服务的话；
        3b 主进程+woker进程，如果必须绑定到一个 tcp port，比如 httpd+fastcgi。
    模式 2 是很多人鄙视的，认为多线程程序难写，而且不比模式 3 有什么优势；
    模式 4 更是千夫所指，它不但没有结合 2 和 3 的优点，反而汇聚了二者的缺点。

模式 2 和模式 3b 的优劣，即：什么时候一个服务器程序应该是多线程的。
从功能上讲，没有什么是多线程能做到而单线程做不到的，反之亦然，都是状态机嘛（我很高兴看到反例）。
从性能上讲，无论是 IO bound 还是 CPU bound 的服务，多线程都没有什么优势。那么究竟为什么要用多线程？
在回答这个问题之前，我先谈谈必须用必须用单线程的场合。

---------------------------------------
3. 必须用单线程的场合
据我所知，有两种场合必须使用单线程：
    程序可能会 fork()
    限制程序的 CPU 占用率
先说 fork()，我在《Linux 新增系统调用的启示》中提到：
    fork() 一般不能在多线程程序中调用，因为 Linux 的 fork() 只克隆当前线程的 thread of control，不克隆
其他线程。也就是说不能一下子fork()出一个和父进程一样的多线程子进程，Linux也没有forkall()这样的系统调用。
forkall()其实也是很难办的（从语意上），因为其他线程可能等在 condition variable上，可能阻塞在系统调用上，
可能等着 mutex 以跨入临界区，还可能在密集的计算中，这些都不好全盘搬到子进程里。

    更为糟糕的是，如果在 fork() 的一瞬间某个别的线程 a 已经获取了 mutex，由于 fork() 出的新进程里
没有这个“线程a”，那么这个 mutex 永远也不会释放，新的进程就不能再获取那个 mutex，否则会死锁。
（这一点仅为推测，还没有做实验，不排除 fork() 会释放所有 mutex 的可能。）
    综上，一个设计为可能调用 fork() 的程序必须是单线程的，比如我在《启示》一文中提到的“看门狗进程”。
多线程程序不是不能调用 fork()，而是这么做会遇到很多麻烦，我想不出做的理由。
    一个程序 fork() 之后一般有两种行为：
    1. 立刻执行 exec()，变身为另一个程序。例如 shell 和 inetd；又比如 lighttpd fork() 出子进程，
然后运行 fastcgi 程序。或者集群中运行在计算节点上的负责启动 job 的守护进程（即我所谓的“看门狗进程”）。
    2. 不调用 exec()，继续运行当前程序。要么通过共享的文件描述符与父进程通信，协同完成任务；要么接过
父进程传来的文件描述符，独立完成工作，例如 80 年代的 web 服务器 NCSA httpd。
# 这些行为中，我认为只有“看门狗进程”必须坚持单线程，其他的均可替换为多线程程序（从功能上讲）。
# 单线程程序能限制程序的 CPU 占用率。
    这个很容易理解，比如在一个 8-core 的主机上，一个单线程程序即便发生 busy-wait（无论是因为 bug 还是因为 
overload），其 CPU 使用率也只有 12.5%，即占满 1 个 core。在这种最坏的情况下，系统还是有 87.5% 的计算资源
可供其他服务进程使用。
    因此对于一些辅助性的程序，如果它必须和主要功能进程运行在同一台机器的话（比如它要监控其他服务进程的状态），
那么做成单线程的能避免过分抢夺系统的计算资源。

---------------------------------------
4. 基于进程的分布式系统设计
    《常用模型》一文提到，分布式系统的软件设计和功能划分一般应该以“进程”为单位。我提倡用多线程，并不是说
把整个系统放到一个进程里实现，而是指功能划分之后，在实现每一类服务进程时，在必要时可以借助多线程来提高性能。
对于整个分布式系统，要做到能 scale out，即享受增加机器带来的好处。
    对于上层的应用而言，每个进程的代码量控制在 10 万行 C++ 以下，这不包括现成的 library 的代码量。这样每个
进程都能被一个脑子完全理解，不会出现混乱。（其实我更想说 5 万行。）

    这里推荐一篇 Google 的好文《Introduction to Distributed System Design》。其中点睛之笔是：
# 分布式系统设计，是 design for failure。

本文继续讨论一个服务进程什么时候应该用多线程，先说说单线程的优势。

---------------------------------------
5. 单线程程序的优势
从编程的角度，单线程程序的优势无需赘言：简单。程序的结构一般如《常用模型》所言，是一个基于 
IO multiplexing 的 event loop。或者如云风所言，直接用阻塞 IO。

event loop 的典型代码框架是：

while ((!done)) { 
  int retval = ::poll(fds, nfds, timeout_ms); 
  if ((retval < 0)) { 
    处理错误 
  } else { 
    处理到期的 timers 
    if ((retval > 0)) { 
      处理 IO 事件 
    } 
  } 
}

    event loop 有一个明显的缺点，它是非抢占的(non-preemptive)。假设事件 a 的优先级高于事件 b，处理事件 a 需要
1ms，处理事件 b 需要 10ms。如果事件 b 稍早于 a 发生，那么当事件 a 到来时，程序已经离开了 poll() 调用开始处理
事件 b。事件 a 要等上 10ms 才有机会被处理，总的响应时间为 11ms。这等于发生了优先级反转。
# 这可缺点可以用多线程来克服，这也是多线程的主要优势。

---------------------------------------
6. 多线程程序有性能优势吗？
    前面我说，无论是 IO bound 还是 CPU bound 的服务，多线程都没有什么绝对意义上的性能优势。这里详细阐述一下
这句话的意思。

    这句话是说，如果用很少的 CPU 负载就能让的 IO 跑满，或者用很少的 IO 流量就能让 CPU 跑满，那么多线程没
啥用处。举例来说：
    1. 对于静态 web 服务器，或者 ftp 服务器，CPU 的负载较轻，主要瓶颈在磁盘 IO 和网络 IO。这时候往往一个
单线程的程序（模式 1）就能撑满 IO。用多线程并不能提高吞吐量，因为 IO 硬件容量已经饱和了。同理，这时增加 
CPU 数目也不能提高吞吐量。
    2. CPU 跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是 n 个整数，问能否从中
选出 m 个整数，使其和为 0 （这里 n < 100, m > 0）。这是著名的 subset sum 问题，是 NP-Complete 的。对于
这样一个“服务”，哪怕很小的 n 值也会让 CPU 算死，比如 n = 30，一次的输入不过 120 字节（32-bit 整数），
CPU 的运算时间可能长达几分钟。对于这种应用，模式 3a 是最适合的，能发挥多核的优势，程序也简单。
#　也就是说，无论任何一方早早地先到达瓶颈，多线程程序都没啥优势。
#  说到这里，可能已经有读者不耐烦了：你讲了这么多，都在说单线程的好处，那么多线程究竟有什么用？

适用多线程程序的场景
---------------------------------------
# 我认为多线程的适用场景是：提高响应速度，让 IO 和“计算”相互重叠，降低 latency。
    虽然多线程不能提高绝对性能，但能提高平均响应性能。
一个程序要做成多线程的，大致要满足：

1. 有多个 CPU 可用。单核机器上多线程的优势不明显。
2. 线程间有共享数据。如果没有共享数据，用模型 3b 就行。虽然我们应该把线程间的共享数据降到最低，
   但不代表没有；
3. 共享的数据是可以修改的，而不是静态的常量表。如果数据不能修改，那么可以在进程间用 shared memory，
   模式 3 就能胜任；
4. 提供非均质的服务。即，事件的响应有优先级差异，我们可以用专门的线程来处理优先级高的事件。防止优先级反转；
5. latency 和 throughput 同样重要，不是逻辑简单的 IO bound 或 CPU bound 程序；
6. 利用异步操作。比如 logging。无论往磁盘写 log file，还是往 log server 发送消息都不应该阻塞 critical path；
7. 能 scale up。一个好的多线程程序应该能享受增加 CPU 数目带来的好处，目前主流是 8 核，很快就会用到 16 核的
   机器了。
8. 具有可预测的性能。随着负载增加，性能缓慢下降，超过某个临界点之后急速下降。线程数目一般不随负载变化。
9. 多线程能有效地划分责任与功能，让每个线程的逻辑比较简单，任务单一，便于编码。而不是把所有逻辑都塞到一个
   event loop 里，就像 Win32 SDK 程序那样。
这些条件比较抽象，这里举一个具体的（虽然是虚构的）例子。

#######################################
    假设要管理一个 Linux 服务器机群，这个机群里有 8 个计算节点，1 个控制节点。机器的配置都是一样的，双路四核 
CPU，千兆网互联。现在需要编写一个简单的机群管理软件（参考 LLNL 的 SLURM），这个软件由三个程序组成：

1. 运行在控制节点上的 master，这个程序监视并控制整个机群的状态。
2. 运在每个计算节点上的 slave，负责启动和终止 job，并监控本机的资源。
3. 给最终用户的 client 命令行工具，用于提交 job。
4. 根据前面的分析，slave 是个“看门狗进程”，它会启动别的 job 进程，因此必须是个单线程程序。另外它不应该
   占用太多的 CPU 资源，这也适合单线程模型。

master 应该是个模式 2 的多线程程序：

1. 它独占一台 8 核的机器，如果用模型 1，等于浪费了 87.5% 的 CPU 资源。
2. 整个机群的状态应该能完全放在内存中，这些状态是共享且可变的。如果用模式 3，那么进程之间的状态同步会成大问题。而如果大量使用共享内存，等于是掩耳盗铃，披着多进程外衣的多线程程序。
3. master 的主要性能指标不是 throughput，而是 latency，即尽快地响应各种事件。它几乎不会出现把 IO 或 CPU 跑满的情况。
4. master 监控的事件有优先级区别，一个程序正常运行结束和异常崩溃的处理优先级不同，计算节点的磁盘满了和机箱温度过高这两种报警条件的优先级也不同。如果用单线程，可能会出现优先级反转。
5. 假设 master 和每个 slave 之间用一个 TCP 连接，那么 master 采用 2 个或 4 个 IO 线程来处理 8 个 TCP connections 能有效地降低延迟。
6. master 要异步的往本地硬盘写 log，这要求 logging library 有自己的 IO 线程。
7. master 有可能要读写数据库，那么数据库连接这个第三方 library 可能有自己的线程，并回调 master 的代码。
8. master 要服务于多个 clients，用多线程也能降低客户响应时间。也就是说它可以再用 2 个 IO 线程专门处理和 clients 的通信。
9. master 还可以提供一个 monitor 接口，用来广播 (pushing) 机群的状态，这样用户不用主动轮询 (polling)。这个功能如果用单独的线程来做，会比较容易实现，不会搞乱其他主要功能。
10. master 一共开了 10 个线程：
    4 个用于和 slaves 通信的 IO 线程
    1 个 logging 线程
    1 个数据库 IO 线程
    2 个和 clients 通信的 IO 线程
    1 个主线程，用于做些背景工作，比如 job 调度
    1 个 pushing 线程，用于主动广播机群的状态
虽然线程数目略多于 core 数目，但是这些线程很多时候都是空闲的，可以依赖 OS 的进程调度来保证可控的延迟。
综上所述，master 用多线程方式编写是自然且高效的。

---------------------------------------
7. 线程的分类
据我的经验，一个多线程服务程序中的线程大致可分为 3 类：
1. IO 线程，这类线程的的主循环是 io multiplexing，等在 select/poll/epoll 系统调用上。这类线程也处理定时事件。
   当然它的功能不止 IO，有些计算也可以放入其中。
2. 计算线程，这类线程的主循环是 blocking queue，等在 condition variable 上。这类线程一般位于 thread pool 中。
3. 第三方库所用的线程，比如 logging，又比如 database connection。
服务器程序一般不会频繁地启动和终止线程。甚至，在我写过的程序里，create thread 只在程序启动的时候调用，
在服务运行期间是不调用的。

}

POSIX(多线程服务器的适用场合:例释与答疑){
1. Linux 能同时启动多少个线程？
    对于 32-bit Linux，一个进程的地址空间是 4G，其中用户态能访问 3G 左右，而一个线程的默认栈 (stack) 
大小是 10M，心算可知，一个进程大约最多能同时启动 300 个线程。如果不改线程的调用栈大小的话，300 左右
是上限，因为程序的其他部分（数据段、代码段、堆、动态库、等等）同样要占用内存（地址空间）。
    对于 64-bit 系统，线程数目可大大增加，具体数字我没有测试，因为我实际用不到那么多线程。
    以下的关于线程数目的讨论以 32-bit Linux 为例。
---------------------------------------
2. 多线程能提高并发度吗？
如果指的是“并发连接数”，不能。
    由问题 1 可知，假如单纯采用 thread per connection 的模型，那么并发连接数最多 300，
这远远低于基于事件的单线程程序所能轻松达到的并发连接数（几千上万，甚至几万）。所谓“基于事件”，
指的是用 IO multiplexing event loop 的编程模型，又称 Reactor 模式，在《常用模型》一文中已有介绍。
    那么采用《常用模型》一文中推荐的 event loop per thread 呢？至少不逊于单线程程序。
    小结：thread per connection 不适合高并发场合，其 scalability 不佳。event loop per thread 的并发度
不比单线程程序差。
---------------------------------------
3. 多线程能提高吞吐量吗？
对于计算密集型服务，不能。
    假设有一个耗时的计算服务，用单线程算需要 0.8s。在一台 8 核的机器上，我们可以启动 8 个线程一起对外服务
（如果内存够用，启动 8 个进程也一样）。这样完成单个计算仍然要 0.8s，但是由于这些进程的计算可以同时进行，
理想情况下吞吐量可以从单线程的 1.25cps （calc per second） 上升到 10cps。（实际情况可能要打个八折――如果
不是打对折的话。）

    假如改用并行算法，用 8 个核一起算，理论上如果完全并行，加速比高达 8，那么计算时间是 0.1s，吞吐量还是 
10cps，但是首次请求的响应时间却降低了很多。实际上根据 Amdahl law，即便算法的并行度高达 95%，8 核的加速比
也只有 6，计算时间为 0.133s，这样会造成吞吐量下降为 7.5cps。不过以此为代价，换得响应时间的提升，在有些
应用场合也是值得的。
    这也回答了问题 4。

    如果用 thread per request 的模型，每个客户请求用一个线程去处理，那么当并发请求数大于某个临界值 T’ 时，
吞吐量反而会下降，因为线程多了以后上下文切换的开销也随之增加（分析与数据请见《A Design Framework for Highly 
Concurrent Systems》 by Matt Welsh et al.）。thread per request 是最简单的使用线程的方式，编程最容易，
简单地把多线程程序当成一堆串行程序，用同步的方式顺序编程，比如 Java Servlet 中，一次页面请求由一个函数
 HttpServlet#service(HttpServletRequest req, HttpServletResponse resp) 同步地完成。

    为了在并发请求数很高时也能保持稳定的吞吐量，我们可以用线程池，线程池的大小应该满足“阻抗匹配原则”，
见问题 7。

    线程池也不是万能的，如果响应一次请求需要做比较多的计算（比如计算的时间占整个 response time 的 1/5 强），
那么用线程池是合理的，能简化编程。如果一次请求响应中，thread 主要是在等待 IO，那么为了进一步提高吞吐，往往
要用其它编程模型，比如 Proactor，见问题 8。
---------------------------------------
4. 多线程能降低响应时间吗？
如果设计合理，充分利用多核资源的话，可以。在突发 (burst) 请求时效果尤为明显。

#######################################
例1: 多线程处理输入。
以 memcached 服务端为例。memcached 一次请求响应大概可以分为 3 步：
1. 读取并解析客户端输入
2. 操作 hashtable
3. 返回客户端

    在单线程模式下，这 3 步是串行执行的。在启用多线程模式时，它会启用多个输入线程（默认是 4 个），并在
建立连接时按 round-robin 法把新连接分派给其中一个输入线程，这正好是我说的 event loop per thread 模型。
这样一来，第 1 步的操作就能多线程并行，在多核机器上提高多用户的响应速度。第 2 步用了全局锁，还是单线程的，
这可算是一个值得继续改进的地方。

    比如，有两个用户同时发出了请求，这两个用户的连接正好分配在两个 IO 线程上，那么两个请求的第 1 步操作
可以在两个线程上并行执行，然后汇总到第 2 步串行执行，这样总的响应时间比完全串行执行要短一些（在“读取并解析”
所占的比重较大的时候，效果更为明显）。请继续看下面这个例子。

#######################################
例2: 多线程分担负载。
    假设我们要做一个求解 Sudoku 的服务（见《谈谈数独》），这个服务程序在 9981 端口接受请求，输入为一行 81 
个数字（待填数字用 0 表示），输出为填好之后的 81 个数字 (1 ~ 9)，如果无解，输出 “NO/r/n”。
    由于输入格式很简单，用单个线程做 IO 就行了。先假设每次求解的计算用时 10ms，用前面的方法计算，单线程程
序能达到的吞吐量上限为 100req/s，在 8 核机器上，如果用线程池来做计算，能达到的吞吐量上限为 800req/s。
下面我们看看多线程如何降低响应时间。

    假设 1 个用户在极短的时间内发出了 10 个请求，如果用单线程“来一个处理一个”的模型，这些 reqs 会排在队列
里依次处理（这个队列是操作系统的 TCP 缓冲区，不是程序里自己的任务队列）。在不考虑网络延迟的情况下，第 1 
个请求的响应时间是 10ms；第 2 个请求要等第 1 个算完了才能获得 CPU 资源，它等了 10ms，算了 10ms，响应时间是 
20ms；依次类推，第 10 个请求的响应时间为 100ms；10个请求的平均响应时间为 55ms。
    如果 Sudoku 服务在每个请求到达时开始计时，会发现每个请求都是 10ms 响应时间，而从用户的观点，10 个请求
的平均响应时间为 55ms，请读者想想为什么会有这个差异。

    下面改用多线程：1 个 IO 线程，8 个计算线程（线程池）。二者之间用 BlockingQueue 沟通。同样是 10 
个并发请求，第 1 个请求被分配到计算线程1，第 2 个请求被分配到计算线程 2，以此类推，直到第 8 个请求被第
 8 个计算线程承担。第 9 和第 10 号请求会等在 BlockingQueue 里，直到有计算线程回到空闲状态才能被处理。
 （请注意，这里的分配实际上是由操作系统来做，操作系统会从处于 Waiting 状态的线程里挑一个，不一定是 
 round-robin 的。）
    这样一来，前 8 个请求的响应时间差不多都是 10ms，后 2 个请求属于第二批，其响应时间大约会是 20ms，
总的平均响应时间是 12ms。可以看出比单线程快了不少。
    由于每道 Sudoku 题目的难度不一，对于简单的题目，可能 1ms 就能算出来，复杂的题目最多用 10ms。那么
线程池方案的优势就更明显，它能有效地降低“简单任务被复杂任务压住”的出现概率。
以上举的都是计算密集的例子，即线程在响应一次请求时不会等待 IO，下面谈谈更复杂的情况.

---------------------------------------
5. 多线程程序如何让 IO 和“计算”相互重叠，降低 latency？
基本思路是，把 IO 操作（通常是写操作）通过 BlockingQueue 交给别的线程去做，自己不必等待。

#######################################
例1: logging
    在多线程服务器程序中，日志 (logging) 至关重要，本例仅考虑写 log file 的情况，不考虑 log server。
    在一次请求响应中，可能要写多条日志消息，而如果用同步的方式写文件（fprintf 或 fwrite），多半会降低性能，
因为：
    文件操作一般比较慢，服务线程会等在 IO 上，让 CPU 闲置，增加响应时间。
    就算有 buffer，还是不灵。多个线程一起写，为了不至于把 buffer 写错乱，往往要加锁。这会让服务线程互相等待，
降低并发度。（同时用多个 log 文件不是办法，除非你有多个磁盘，且保证 log files 分散在不同的磁盘上，否则还是
受到磁盘 IO 瓶颈制约。）

    解决办法是单独用一个 logging 线程，负责写磁盘文件，通过一个或多个 BlockingQueue 对外提供接口。别的线程
要写日志的时候，先把消息（字符串）准备好，然后往 queue 里一塞就行，基本不用等待。这样服务线程的计算就和 
logging 线程的磁盘 IO 相互重叠，降低了服务线程的响应时间。

    尽管 logging 很重要，但它不是程序的主要逻辑，因此对程序的结构影响越小越好，最好能简单到如同一条 printf 
语句，且不用担心其他性能开销，而一个好的多线程异步 logging 库能帮我们做到这一点。（Apache 的 log4cxx 和 
log4j 都支持 AsyncAppender 这种异步 logging 方式。）

-----------------------------------------
7. 什么是线程池大小的阻抗匹配原则？
我在《常用模型》中提到“阻抗匹配原则”，这里大致讲一讲。
还有就是ACE下的CPU数量+1这样的模式，不是浪费系统的资源吗？

-----------------------------------------
8. 除了你推荐的 reactor + thread poll，还有别的 non-trivial 多线程编程模型吗？
有，Proactor。

    如果一次请求响应中要和别的进程打多次交道，那么 proactor 模型往往能做到更高的并发度。
当然，代价是代码变得支离破碎，难以理解。
    这里举 http proxy 为例，一次 http proxy 的请求如果没有命中本地 cache，那么它多半会：
             # asynchronous dns，有开源的库。另外 libevent 也支持异步 DNS 解析。
1. 解析域名 （不要小看这一步，对于一个陌生的域名，解析可能要花半秒钟）
2. 建立连接
3. 发送 HTTP 请求
4. 等待对方回应
5. 把结果返回客户

这 5 步里边跟 2 个 server 发生了 3 次 round-trip：

1. 向 DNS 问域名，等待回复；
2. 向对方 http 服务器发起连接，等待 TCP 三路握手完成；
3. 向对方发送 http request，等待对方 response。

而实际上 http proxy 本身的运算量不大，如果用线程池，池中线程的数目会很庞大，不利于操作系统管理调度。
}

POSIX(阿姆达尔定律Amdahl law){
    阿姆达尔定律（英语：Amdahl's law，Amdahl's argument），一个计算机科学界的经验法则，因吉恩・阿姆达尔而得名。
它代表了处理器平行运算之后效率提升的能力。
    并行计算中的加速比是用并行前的执行速度和并行后的执行速度之比来表示的，它表示了在并行化之后的效率提升情况。

}
# https://blog.codingnow.com/2006/04/iocp_kqueue_epoll.html
POSIX(IOCP , kqueue , epoll ... 有多重要？){
    设计 mmo 服务器，我听过许多老生常谈，说起处理大量连接时， select 是多么低效。我们应该换用 iocp (windows),
kqueue(freebsd), 或是 epoll(linux) 。的确，处理大量的连接的读写，select 是够低效的。因为 kernel 每次都要对 
select 传入的一组 socket 号做轮询，那次在上海，以陈榕的说法讲，这叫鬼子进村策略。一遍遍的询问“鬼子进村了吗？”，
“鬼子进村了吗？”... 大量的 cpu 时间都耗了进去。（更过分的是在 windows 上，还有个万恶的 64 限制。）

    使用 kqueue 这些，变成了派一些个人去站岗，鬼子来了就可以拿到通知，效率自然高了许多。不过最近我在反思，真的
需要以这些为基础搭建服务器吗？
刚形成的一个思路是这样的：
    我们把处理外部连接和处理游戏逻辑分摊到两个服务器上处理，为了后文容易表述，暂时不太严谨的把前者称为连接服务
器，后者叫做逻辑服务器。
    连接服务器做的事情可以非常简单，只是把多个连接上的数据汇集到一起。假设同时连接总数不超过 65536 个，我们
只需要把每个连接上的数据包加上一个两字节的数据头就可以表识出来。这个连接服务器再通过单个连接和逻辑服务器通讯
就够了。
    那么连接服务器尽可以用最高效的方式处理数据，它的逻辑却很简单，代码量非常的小。而逻辑服务器只有一个外部连接，
无论用什么方式处理都不会慢了。

    进一步，我们可以把这个方法扩展开。假定我们逻辑以 10Hz 的频率处理逻辑。我们就让连接服务器以 10Hz 的脉冲把
汇总的数据周期性的发送过去，先发一个长度信息再发数据包。即使一个脉冲没有外部数据，也严格保证至少发一个 0 的
长度信息。额外的，连接服务器还需要控制每个脉冲的数据总流量，不至于一次发送数据超过逻辑服务器处理的能力。
    那么，逻辑服务器甚至可以用阻塞方式调用 recv 收取这些数据，连 select 也省了。至于数据真的是否会被接收方
阻塞，就由连接服务器的逻辑保证了。

    说到阻塞接收，我跟一个同事讨论的时候，他严重担心这个的可靠性，不希望因为意外把逻辑服务器挂在一个 
system call 上。他列举了许多可能发生的意外情况，不过我个人是不太担心的，原因不想在这里多解释。当然我这样设计，
主要不是为了节省一个 select 的调用，而是希望方便调试。（当然，如果事实证明这样不可行，修改方案也很容易）

    因为阻塞接收可以保证逻辑服务器的严格时序性，当我们把两个服务器中的通讯记录下来，以后可以用这些数据完全
重现游戏逻辑的过程，无论怎么调试运行，都可以保证逻辑服务器的行为是可以完全重现的。即，每 0.1s 接受已知的
数据包，然后处理它们。
    这样做，逻辑服务器对网络层的代码量的需求也大大减少了，可以更专心的构建逻辑。
    
}
POSIX(Linux 新增系统调用的启示){
1. 服务器程序的风格可能在变
新的创建文件描述符的 syscall 一般都支持额外的 flags 参数，可以直接指定 O_NONBLOCK 和 FD_CLOEXEC，例如：
    accept4 C 2.6.28
    eventfd2 C 2.6.27
    inotify_init1 C 2.6.27
    pipe2 C 2.6.27
    signalfd4 C 2.6.27
    timerfd_create 2.6.25
以上 6 个 syscalls，除了最后一个是新功能，其余的都是增强原有的调用，把数字尾号去掉就是原来的 syscall。
O_NONBLOCK 的功能是开启“非阻塞IO”，而文件描述符默认是阻塞的。

    这些创建文件描述符的系统调用能直接设定 O_NONBLOCK 选项，或许能反映当前 Linux （服务端）开发的风向，
那就是我在前一篇博客《多线程服务器的常用编程模型》里推荐的 one loop per thread + (non-blocking IO with 
IO multiplexing)。从这些内核改动来看，non-blocking IO 已经主流到让内核增加 syscall 以节省一次 fcntl(2) 
调用的程度了。

另外，以下新系统调用可以在创建文件描述符时开启 FD_CLOEXEC 选项：
dup3 C 2.6.27
epoll_create1 C 2.6.27
而文件描述默认是被子进程继承的（这是传统 Unix 的一种典型 IPC，比如用 pipe(2) 在父子进程间单向通信）。
    以上 8 个新 syscalls 都允许直接指定 FD_CLOEXEC，或许说明 fork() 的主要目的已经不再是创建 worker process 
并通过共享的文件描述符和父进程保持通信，而是像 Windows 的 CreateProcess 那样创建“干净”的进程，其与父进程没有
多少瓜葛。

    以上两个 flags 在我看来，说明 Linux 服务器开发的主流模型正在由 fork() + worker processes 模型转变为
我前文推荐的多线程模型。fork() 的使用频度会大大降低，将来或许只有专门负责启动别的进程的“看门狗程序”才会
调用 fork()，而一般的服务器程序（此处“服务器程序”的定义见我前一篇文章）不会再 fork() 出子进程了。
    原因之一是，fork() 一般不能在多线程程序中调用，因为 Linux 的 fork() 只克隆当前线程的 thread of control，
不克隆其他线程。也就是说不能一下子 fork() 出一个和父进程一样的多线程子进程，Linux 没有 forkall() 这样的系统
调用。forkall() 其实也是很难办的（从语意上），因为其他线程可能等在 condition variable 上，可能阻塞在系统
调用上，可能等这 mutex 以跨入临界区，还可能在密集的计算中，这些都不好全盘搬到子进程里。由此可见，“看门狗程序”
应该是单进程的，而且能捕获 SIGCHLD，如果 signal 能像“文件”一样读就能大大简化开发，下面第 2 点正好印证了。

# 既然如此，那么在 fork() 时关闭不相干的文件描述符就成了常见的需求，干脆做到系统调用里得了。

2. Kernel 2.6.22 加入的 signalfd 让 signal handling 有了新办法。
---------------------------------------
    signal 处理是 Unix 编程的难点，因为 signal 是异步的，而且发生在“当前线程”里，会遇到“可重入”的难题。
其实“线程”是 1993 才加入到 Unix 中，之前的 20 多年根本就没有“主线程”一说，我这里的意思是 signal handler 
是像 coroutine 一样被调用的，而不是通常的 subroutine。Raymond Chen 有一篇文章谈到了这个问题。

    在 Unix/Linux 支持线程以后，signal 就更难处理了，规则变得晦涩（想想 signal delivery 的对象）。而且它
不符合“every thing is a file” 的 Unix 哲学，不能把 signal 事件当成文件来读。不过 2.6.22 加入的 signalfd 
让事情有了转机，程序能像处理文件一样处理 signal，可以 read，也可以 select/poll/epoll，能融入标准的 IO 
multiplexing 框架中，而不需要在程序里另外用一对 pipe 来把 signal 转为 IO event。（libev 似乎是这么做的，
另外还有 GHC http://hackage.haskell.org/trac/ghc/ticket/1520 ）

# 这下多线程程序与 signals 打交道容易多了，一个 event loop 就能搞定 IO 和 timer 和 signals，完美。

3. Kernel 2.6.25 加入的 timerfd 让程序的“定时任务”有了新办法。
---------------------------------------
    我下一篇博客会详细分析 Linux 服务器程序中的日期与时间，其中一块内容是“定时”，也就是程序借助定时器在未来
某个时刻做特定的事情。在 Linux 下办法很多，基于阻塞的 sleep/nanosleep/clock_nanosleep, 基于 signals 的 
rtsignal/timer_create，还有我喜欢的基于 IO multiplexing 的 poll/epoll。不过 poll/epoll 的理论定时精度最多
只有毫秒（函数的参数就是毫秒数，不能指定更高的时间精度），实际等待精度取决于 kernel HZ 等。

    如果需要在 event loop 里做无阻塞的高精度定时，现在可以用 timerfd 了。而且它既然是个 fd，就能很方便地和 
non-blocking IO 与 IO multiplexing 融合到一起，浑然天成。当然，文件描述符是稀缺资源，如果每个 event loop 
都采用 timerfd 来做 timer/timeout 似乎是一种浪费（每个 timer 一个 timerfd 更是巨大浪费，因为不是每个 timer 
都需要高精度定时）, # 我宁愿采用传统的优先队列办法来管理等待到期的 timers（毫秒级的定时精度已经能满足我的需要），
# 只在特殊场合动用 timerfd。

4. Kernel 2.6.22 加入的 eventfd 让“线程间事件通知”有了新办法。
---------------------------------------
    《多线程服务器的常用编程模型》 提到进程间通信只用 TCP，而 pipe 的惟一作用是异步唤醒 event loop，现在有了
eventfd，pipe 连这个作用都没有了。eventfd 是一个比 pipe 更高效的线程间事件通知机制，一方面它比 pipe 少用一个 
file descriper，节省了资源；另一方面，eventfd 的缓冲区管理也简单得多，全部“buffer”一共只有 8 bytes，不像 
pipe 那样可能有不定长的真正 buffer。
# pipe 将来的作用或许主要是被“看门狗程序”用来截获子进程的 stdout/stderr。

# FD_CLOEXEC 的功能是让程序 fork() 时，子进程会自动关闭这个文件描述符”，这是错误的，FD_CLOEXEC 顾名思义是
# 在执行 exec() 调用时关闭文件描述符，防止文件描述符泄漏给子进程。我对fork()的第一反应是立即执行exec()，

}

POSIX(Sleep 反模式){
    我认为 sleep 只能出现在测试代码中，比如写单元测试的时候。（涉及时间的单元测试不那么好写，短的如一两秒钟
可以用 sleep，长的如一小时一天得想其他办法，比如把算法提出来并把时间注入进去。）产品代码中线程的等待可分为
两种：一种是无所事事的时候（要么等在 select/poll/epoll 上。要么等在 condition variable 上，等待 BlockingQueue
 /CountDownLatch 亦可归入此类），一种是等着进入临界区（等在 mutex 上）以便继续处理。在程序的正常执行中，
 如果需要等待一段时间，应该往 event loop 里注册一个 timer，然后在 timer 的回调函数里接着干活，因为线程是个
 珍贵的共享资源，不能轻易浪费。如果多线程的安全性和效率要靠代码主动调用 sleep 来保证，这是设计出了问题。
 等待一个事件发生，正确的做法是用 select 或 condition variable 或（更理想地）高层同步工具。当然，在 GUI 编程
 中会有主动让出 CPU 的做法，比如调用 sleep(0) 来实现 yield。
}

POSIX(libuv){
libuv很多时候是使用在Web应用这一块的，所以底层性能第一，设计第二，可读性第三.

    个人认为还不如用containof宏来做转换这件事情，反而比第一个分量直接转换来的看的清楚，
反正linux kernel里面也是大把的containof宏。
}

POSIX(Linux C++ 服务器端这条线怎么走？一年半能做出什么？){
1. 学习操作系统的目的，不是让你去发明自己操作系统内核，打败 Linux；也不是成为内核开发人员；而是理解操作系统
   为用户态进程提供了怎样的运行环境，作为程序员应该如何才能充分利用好这个环境，哪些做法是有益的，哪些是做
   无用功，哪些则是帮倒忙。
2. 学习计算机体系结构的目的，不是让你去设计自己的 CPU（新的 ISA 或微架构），打败 Intel 和 ARM；也不是参与到 
   CPU 设计团队，改进现有的微架构；而是明白现代的处理器的能力与特性（例如流水线、多发射、分支预测、乱序执行
   等等指令级并行手段，内存局部性与 cache，多处理器的内存模型、能见度、重排序等等），在编程的时候通过适当
   组织代码和数据来发挥 CPU 的效能，避免 pitfalls。
   # Modern Microprocessors
   # http://www.lighterra.com/papers/modernmicroprocessors/
#######################################
这两门课程该如何学？看哪些书？这里我告诉你一个通用的办法，去美国计算机系排名靠前的大学的课程主页，找到这
两门课最近几年的课程大纲、讲义、参考书目、阅读材料、随堂练习、课后作业、编程实验、期末项目等，然后你就
心里有数了。
}
poco(世界上有两种网络编程：网络编程和Windows网络编程){
}
C++(){
C++之父Bjarne Stroustrup的代表作《The C++ Programming Language》
和Stan Lippman的这本《C++ Primer》

----------------------------------------
这是一本C++语言的教程，不是编程教程。
}