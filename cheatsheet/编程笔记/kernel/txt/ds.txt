data struct == data set + operators
as a user if u donno the inner implementation of the 2 above, the data struct is abstract data struct(ADT)


list average checking time	number of nodes/2

bin-tree's array size is always 2^n-1, while maximum acount is 2^n-1, while index starts from 1

pre-order	father - left - right
mid-order	left - father - right
post-order	left - right - father

拓扑排序	
Directed Acyclic Graph(DAG)有向无环图
顶点按边的方向出现的先后顺序;如果有环，则无法表示两个顶点的先后顺序

|E| < |V|log|V|		sparse / direct		list
otherwise		dense  / undirect	matrix

list可以无限调整大小，array只能在最开始确定大小

perfect hash function -- give a data, get a hash value, no 2 different data have the same calculated_val

optimization:	I/O, memory-management
printf, malloc, free

table_size 互质 check_footstep --> every slot will be checked

hash_key	hash_table's index
hash_func	give a data, get a calculated_val, maybe a random
hash_table	storage array
hash_key = calculated_val % hash_table_size(PRIME)	random % 100 == 0-99

the least one that is larger than A while is multiple of B
(A + B - 1) / B * B

动态数组支持多种高效的排序算法，像快速排序、归并排序和堆排序等等，而这些算法在
双向链表中的表现并不好，甚至不如冒泡排序来得快。

排序好的动态数组可以使用二分查找，而排序好的双向链表仍然只能使用顺序查找。主要
原因是双向链表不支持随机定位，定位中间结点时只能一个一个的移动指针

(1) 时间频度是一个算法执行所耗费的时间，从理论上是不能算出来的，必须上机运行测试才能知道。但我们不可能也没有必要对每个算法都上机测试，只需知道哪个算法花费的时间多，哪个算法花费的时间少就可以了。并且一个算法花费的时间与算法中语句的执行次数成正比，哪个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度，记为T(n)。

(2) 时间复杂度 在上面提到的时间频度中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律。为此，我们引入时间复杂度的概念。一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数，记作T(n)=O(f(n))，称O(f(n))为算法的渐进时间复杂度，简称时间复杂度。在各种不同算法中，若算法中语句执行次数为一个常数，则时间复杂度为O(1)，另外，在时间频度不相同时，时间复杂度也可能相同，如与，它们的时间频度不同，但时间复杂度相同，都为。按数量级递增排列，常见的时间复杂度有：常数阶O(1)，对数阶，线性阶O(n)，线性对数阶，平方阶，立方阶，...，k次方阶，指数阶。随着问题规模n的不断增大，上述时间复杂度不断增大，算法的执行效率越低。

bsearch
	ERROR : if find 9 in (1,7,10), low=1/mid=1/high=2, INFINITE!!	
	while(low <= high) {
		mid = (low + high) >> 1;
		if (lis[mid] < wanted) low = mid;
		else if (lis[mid] > wanted) high = mid - 1;
		else return mid;
	}
because integer division would always drop float out, so mid either at the middle or 1 elem left to the middle, namely closer to low
=> sometimes mid may =low, so low must +1; "high=mid-1 is optional"
standard low<=high;low=mid+1;high=mid-1;  if not find, return -1, like bsearch in <stdlib.h>
if want to find the proper position to insert/substitute, like in uva481 use nlogn LIS 
use standard and return low at last is OK


graph
原來 Prim ’ s Algorithm 要特別注意連通。他仔細了想了想，原來 Prim ’ s Algorithm 找到的是 Subtree ，而 Kruskal ’ s Algorithm 找到的是 Forest ，這樣才對
寫最小生成樹要注意連通
所有的圖論演算法都要注意連通

學凸包演算法時，要先了解內積與外積的各種應用

int 型的 key, 用rbtree更快。
hashtable 适合字符串为key的查找

list
    reverse: if no n spaces, need 3 pointers; else delete one from old while add it to the new
    is cycle: 2 pointers, 1 takes 1 step at a time while the other takes 2, special case is "6"

sort:
    multi-fields: qsort callback-func
    swap count: bubble/merge
    merge: stable & inner/outer
