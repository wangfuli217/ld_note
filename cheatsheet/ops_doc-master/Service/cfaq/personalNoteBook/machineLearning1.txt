
网易云课堂《吴恩达机器学习》包含PPT形式的笔记，上面也可以留自己的笔记，笔记会自动对应到视频时间。
这个课程以非常直观的感觉来介绍原理，也介绍实现细节，非常棒。

Octave 是一个免费的开源软件，用于机器学习，快速开发原型，然后证明算法可用之后，再用C++,python实现；Octave是和Matlab类似的软件；

0.T: task , E:experience, P:performance measure.
1.监督学习：输入有答案的数据，每个样本数据都包含输入和输出，其实就是pair<input, output>。监督学习有2类：回归和分类。
  非监督学习：没有答案的数据；比如有一堆数据，不知道有什么用，就可以通过聚类算法进行自动分类，不需要提前告知机器分类策略。
2.拟合：从数据中学习一条曲线或者曲面，提炼出一个函数，该函数尽量表达输出与输入的关系，但这个函数没法用数学公式显示的表达。
   线性拟合： h(x) = ax + b; 
3.回归：用学到的曲线来预测答案，曲线是连续的。
4.分类： 回归和分类的根本区别是：回归问题的输出值是连续的，而分类问题的输出值是离散的。
5.机器学习：能够推导出一个函数，可以泛化到不在训练集上的输入值。
6.实际中输入给机器学习的不是原始数据，而是经过提取的特征，比如角度形状，这样学习更加简单高效。
7.误差反向传播的数学工具：链式求导法则，用于调节各隐层权重。
8.代价函数：平方误差函数，eg: F(a,b)=sum(ax+b - y)^2,  其中(x,y)是样本，a,b是待求的模型参数，我们的目标是最小化代价函数min(F)，用到的工具是梯度下降;
9.梯度下降：最优化的一种手段，梯度就是偏导数，随机给定一个初始值，然后不断沿着梯度方向调整，最终得到的是一个局部最优解。
   更新梯度的时候要用到所有的输入样本，这个算法叫BatchGradientDescent.
   学习率：如果太小，收敛过慢，如果太大，会导致不收敛，所以要适中。

10.求解最优化的2种方式：1）梯度下降，迭代求解，每次迭代可以用向量化并行计算N个参数；2）正规方程组；   
10.工程中常用的迭代求解最优值算法：Gradient Descent, Conjugate Gradient, BFGS, L-BFGS,  梯度下降是最简单的，其它4种更高效，比梯度下降更快，且不需要人工选择学习率，都有现成的库，对于C++语言的库建议多尝试几个，选择效果最好的。

10.逻辑回归：其实是逻辑分类，分成2类，借用了Sigmoid函数，这个函数的一个优良性质是: 1> y >=0.5 if x>=0 else 0<y<0.5,  
     模型函数h(x) = g(z) = g(f(Xi)), g是sigmoid函数，表达的含义是：h(x) = P(y=1|x); 对于给定的样本x,    h(x)表示属于某一类的概率.  属于另一类的概率是1-h(x).  
 所以要分成2类的话可以取0.5作为阈值：h(x) >0.5属于类A，其它的属于类B，分类边界就是z= f(Xi) > 0.这个就是特征空间Xi上的决策边界.
 由h(x)得到的代价函数cost(h, y)=(h-y)^2是非凸函数，为了使用梯度下降法求得全局最优, 必须转为凸函数，所以cost(h,y) = -ylog(h) + (1-y)log(1-h);

11.多分类：逻辑回归可用于多分类，比如分成3类，那需要训练3次，每次得到一个h(x)，总共得到3个h(x), h(x)的含义就是归属于某类的概率，然后对于新样本，判决为 h(x) 最大的那个类; 
  

12.  实现细节：
      a)梯度下降： 在每次迭代的最后一步同时更新所有参数，不要在中间步骤去更新，可以用矩阵进行向量化加速；
      b)逻辑回归训练时，y只能取0或1，如果某类样本对应标签y=1, 则其他样本对应y=0,   则h(x)的含义就是 x 属于这个类别的概率;
      

13.模型该如何构造？ 根据领域经验吗？还是把数据可视化之后观察得到？

14.Octave求最优值函数:
function [jVal, gradient] = costfunction(theta);
options=optimset('GradObj', 'on', 'MaxIter','100'); 
fminunc(@costfunction, initialTheta, options); 

//////////////////数学
0.矩阵的好处是可以利用硬件的SIMD指令并行处理，加快机器学习进度，矩阵运算不需要再去实现，使用一些函数库就够了，这些函数库都是经过高度优化的。
1.逆矩阵： A* A-1 = A-1 * A = E,   只有方正才有逆矩阵，没有逆矩阵的方正叫奇异矩阵，特点是元素趋近0;
2.单位矩阵：主对角线是1，其它是0, E必须是方正，  对任意矩阵A： A*E = E*A = A;
3.矩阵转置：Bji  =  Aij;   行-->列， 列--->行;
4.矩阵相乘：A*H, A中的每行代表1个样本的特征向量，H中的每列Hi代表1个参数向量，也就是1个模型，应该把A*H拆解诚A*[H0, H1,..Hn] = dataMatrix * [h0, h1,,hn] = prediction. 表达的含义   就是同时用多个模型对输入进行预测;
5.矩阵乘法满足结合律： A*B*C = A*(B*C)，但不满足交换律:A*B   != B*A;


////////////////////////////////吴恩达机器学习
《课时10&&11》 梯度下降原理和直观解释。
《课时43-44》逻辑回归，分类边界的直观解释。
