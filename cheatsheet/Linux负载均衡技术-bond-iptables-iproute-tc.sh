bond()
{
    bond是Linux内核自带的多网卡聚合功能。这个功能可以把多个网卡整合成一个虚拟网卡从而同时利用多块网卡传输数据。
bond有多种不同的模式用以适应不同的情况。bond主要是从L2链路层考虑的，因此可以bond的网卡通常要连到同一个交换机上。

bond是linux内核自带的一个多个网卡聚合的功能。这个功能可以以多种方式实现linux下的多个网卡聚合成一个逻辑网卡，
从而实现冗余或者负载均衡。它有以下几种工作模式：
    bond0：轮询。各接口轮流使用。需要交换机支持。交换机要配置成聚合口。
    bond1：主备。即只有一个接口在使用，当接口出现问题的时候再切换到别的接口。
    bond2：xor。根据数据的目的mac地址计算使用的端口。
    bond3：broadcast。所有数据在每个接口都复制一份
    bond4：lacp。适用802.3ad。类似bond2
    bond5：对外出流量负载均衡
    bond6：通过修改ARP包实现负载均衡。即对到来的arp请求依据一定策略使用不通过的arp进行回复。
    总结：可以看出，linux的bond主要是在二层链路层实现负载均衡和冗余（即进行bond的多个接口连的要是同一网段）。
    除此之外，bond的很多模式还需要交换机的支持。
}

iproute2()
{
    可以使用ip命令中的nexthop选项非常简单的实现负载均衡。nexthop可以为同一个目的地址指定多条路由，并可以指定每
条路由的权重。之后系统会根据权重为数据选择某条路由。

例如：ip route add default nexthop via $P1 dev $IF1 weight 1 nexthop via $P2 dev $IF2 weight 1
    这条命令定义默认路由有ip1：if1和ip2：if2两条路径，且这两条路径的权重相同。
    利用ip命令实现上边这样简单的负载均衡是非常方便的，但是ip命令毕竟是一个网络层命令，因此它是无法根据运输层
（端口号）进行负载均衡的。
}

iptables_iproute2()
{
    提起iptables大多数人都只用过其中的filter表和nat表，而对于mangle表则很少有人使用。然后使用mangle表是可以非常方便地
实现灵活负载均衡的。
    iptables的mangle表用来修改数据包的一些标识，可以修改的标识有：dscp（区分服务类型）、ToS（服务类型）、mark（标记）。
前两者是ip头的QoS相关标志位，主要用于ip层的qos实现，但是目前这两个标志位极少被用到，通常都会被忽略。而第三个（mark）
则是由linux内核实现的对数据包的标记。因为是由linux内核实现的，因此这个标记只能在本机使用，并非数据包的一部分，出了
本机这个标记就不存在了。
    iptables实现负载均衡主要是通过修改mark标记来实现。iproute2可以针对数据包的不同mark定义专门的策略路由表，因此
我们可以把去往不同接口的路由写入适用不同mark的策略路由表中，之后linux就可以根据数据包的mark来决定数据包要走的路由了。

例如： 
    首先，为所有进入本机的目的端口号为25的tcp包打上“1”标记：
    iptables -A PREROUTING  -t mangle -p tcp --dport 25 -j MARK --set-mark 1   
    添加路由规则规定标记为1的数据包使用路由表200
    ip rule add fwmark 1 table 200
    在200路由表中添加路由
    ip route add default via 192.168.1.1 dev ppp0 table 200
    同理，为所有进入本机的目的端口号为80的tcp包打上“2”标记
    iptables -A PREROUTING  -t mangle -p tcp --dport 80 -j MARK --set-mark 2
    添加相关路由，方法同上：
    ip rule add fwmark 2 table 200
    ip route add default via 192.168.2.1 dev wlan0 table 200
    这样子一来以后所有的端口号为25的包都会走192.168.1.1 dev ppp0路由，而所有端口号为80的数据包则会走192.168.2.1 
 dev wlan0路由，从而实现了针对端口号的负载均衡。同理，也可以利用iptables实现针对源（目的）地址、协议、接口的
 负载均衡，真的非常的方便。
}


tc()
{
    流量控制器TC（Traffic Control）用于Linux内核的流量控制，它利用队列规定建立处理数据包的队列，并定义队列中的
数据包被发送的方式， 从而实现对流量的控制。TC命令通过建立筛选器（filter）、分类器（class）、队列（qdisc）
这三个对象来实现对流量的控制。具体的讲就是，通过筛选器决定哪些数据包进入到哪些分类的队列中，之后再按照一定的规则
将数据包从各个分类的队列中发送出去。尽管tc的主要功能在于流量控制，但是我们可以通过把一个队列建立在多个网卡上
来实现流量的负载均衡。
例如：
    在eth1上建立一个队列：
    tc qdisc add dev eth1 root teql0
    在eth2上建立同样的队列
    tc qdisc add dev eth2 root teql0
    启动设备teql0 
    ip link set dev teql0 up
    通过这三条命令，所有发往teql0的流量都会在eth1和eth2之间进行负载均衡发送。
    应该说，tc是一个相当强大复杂的工具，但是tc的主要功能还是在于流量控制。

}

LVS()
{
    LVS（Linux virtual machine）是一套集成在Linux内核中的负载均衡服务。LVS通过部署负载均衡服务器在网络层截获并修改
报文并依据一定规则分发给服务器集群中服务器来实现负载均衡。LVS主要用于web服务器的负载均衡，通过LVS，用户的请求可以
被调度到服务器集群的多个服务器上去，并且用户认为自己始终在跟唯一一台服务器进行通信。LVS与前边几种负载均衡技术最大
的差别在于，LVS有非常具体的应用场景，即web服务器集群。

    ipvsadm是LVS在应用层的管理命令，我们可以通过这个命令去管理LVS的配置。在笔者使用的fedora14系统中，已经集成了LVS
相关模块，但是ipvsadm命令仍然需要使用yum单独安装。
}

lvs(基本用法)
{
ipvsadm COMMAND [protocol] service-address [scheduling-method] [persistence options]
ipvsadm command [protocol] service-address server-address [packet-forwarding-method] [weight options]

第一条命令用于向LVS系统中添加一个用于负载均衡的virtual server（VS）；
第二条命令用来修改已经存在的VS的配置，service address用来指定涉及的虚拟服务即虚拟地址，
server-address指定涉及的真实地址。

}

lvs(命令)
{
命令：
    -A, --add-service：为ipvs虚拟服务器添加一个虚拟服务，即添加一个需要被负载均衡的虚拟地址。
                        虚拟地址需要是ip地址，端口号，协议的形式。
    -E, --edit-service：修改一个虚拟服务。
    -D, --delete-service：删除一个虚拟服务。
    -C, --clear：清除所有虚拟服务。
    -R, --restore：从标准输入获取ipvsadm命令。一般结合下边的-S使用。
    -S, --save：从标准输出输出虚拟服务器的规则。可以将虚拟服务器的规则保存，在以后通过-R直接读入，以实现自动化配置。
    -a, --add-server：为虚拟服务添加一个real server（RS）
    -e, --edit-server：修改RS
    -d, --delete-server：删除
    -L, -l, --list：列出虚拟服务表中的所有虚拟服务。可以指定地址。添加-c显示连接表。
    -Z, --zero：将所有数据相关的记录清零。这些记录一般用于调度策略。
    --set tcp tcpfin udp：修改协议的超时时间。
    --start-daemon state：设置虚拟服务器的备服务器，用来实现主备服务器冗余。（注：该功能只支持ipv4）
    --stop-daemon：停止备服务器。
    -h, --help：帮助。
    
}

lvs(选项)
{
以下参数可以接在上边的命令后边。
    -t, --tcp-service service-address：指定虚拟服务为tcp服务。service-address要是host[:port]的形式。
                                       端口是0表示任意端口。如果需要将端口设置为0，还需要加上-p选项（持久连接）。
    -u, --udp-service service-address：使用udp服务，其他同上。
    -f, --fwmark-service integer：用firewall mark取代虚拟地址来指定要被负载均衡的数据包，可以通过这个命令实现
                                  把不同地址、端口的虚拟地址整合成一个虚拟服务，可以让虚拟服务器同时截获处理
                                  去往多个不同地址的数据包。fwmark可以通过iptables命令指定。如果用在ipv6需要加上-6。
    -s, --scheduler scheduling-method：指定调度算法。调度算法可以指定以下8种：
                                       rr（轮询），wrr（权重），lc（最后连接），wlc（权重），lblc（本地最后连接），
                                       lblcr（带复制的本地最后连接），dh（目的地址哈希），sh（源地址哈希），
                                       sed（最小期望延迟），nq（永不排队）
    -p, --persistent [timeout]：设置持久连接，这个模式可以使来自客户的多个请求被送到同一个真实服务器，通常用于ftp或者ssl中。
    -M, --netmask netmask：指定客户地址的子网掩码。用于将同属一个子网的客户的请求转发到相同服务器。
    -r, --real-server server-address：为虚拟服务指定数据可以转发到的真实服务器的地址。可以添加端口号。
                                      如果没有指定端口号，则等效于使用虚拟地址的端口号。
    [packet-forwarding-method]：此选项指定某个真实服务器所使用的数据转发模式。需要对每个真实服务器分别指定模式。
        -g, --gatewaying：使用网关（即直接路由），此模式是默认模式。
        -i, --ipip：使用ipip隧道模式。
        -m, --masquerading：使用NAT模式。
    -w, --weight weight:设置权重。权重是0~65535的整数。如果将某个真实服务器的权重设置为0，那么它不会收到新的连接，
                        但是已有连接还会继续维持（这点和直接把某个真实服务器删除时不同的）。
    -x, --u-threshold uthreshold：设置一个服务器可以维持的连接上限。0~65535。设置为0表示没有上限。
    -y, --l-threshold lthreshold：设置一个服务器的连接下限。当服务器的连接数低于此值的时候服务器才可以重新接收连接。
                                  如果此值未设置，则当服务器的连接数连续三次低于uthreshold时服务器才可以接收到新的连接。
                                  （PS：笔者以为此设定可能是为了防止服务器在能否接收连接这两个状态上频繁变换）
    --mcast-interface interface：指定使用备服务器时候的广播接口。
    --syncid syncid：指定syncid，同样用于主备服务器的同步。
    以下选项用于list命令：

        -c, --connection：列出当前的IPVS连接。
        --timeout：列出超时
        --daemon：
        --stats：状态信息
        --rate：传输速率
        --thresholds：列出阈值
        --persistent-conn：坚持连接
        --sor：把列表排序。
        --nosort：不排序
        -n, --numeric：不对ip地址进行dns查询
        --exact：单位

    -6：如果fwmark用的是ipv6地址需要指定此选项。
}

其他注意事项
    如果使用IPv6地址，需要在地址两端加上”【】“。例如：ipvsadm -A -t [2001:db8::80]:80 -s rr
    可以通过设置以下虚拟文件的值来防御DoS攻击：/proc/sys/net/ipv4/vs/drop_entry 
                                               /proc/sys/net/ipv4/vs/drop_packet 
                                               /proc/sys/net/ipv4/vs/secure_tcp
    
lvs(使用NAT模式)
{
    添加地址为207.175.44.110:80的虚拟服务，指定调度算法为轮转。
ipvsadm -A -t 207.175.44.110:80 -s rr
添加真实服务器，指定传输模式为NAT
ipvsadm -a -t 207.175.44.110:80 -r 192.168.10.1:80 -m
ipvsadm -a -t 207.175.44.110:80 -r 192.168.10.2:80 -m
ipvsadm -a -t 207.175.44.110:80 -r 192.168.10.3:80 -m
NAT模式是lvs的三种模式中最简单的一种。此种模式下只需要保证调度服务器与真实服务器互通就可以运行。
}
    
lvs(使用DR模式)
{
对于DR模式首先要配置真实服务器：
对于每台真实服务器要进行以下操作：
1、设置真实服务器的lo接口不做ARP应答
    echo 1 > /proc/sys/net/ipv4/conf/all/arg_ignore
    echo 1 > /proc/sys/net/ipv4/conf/lo/arg_ignore
    设置这个选项可以使得各个接口只对本接口上的地址进行响应
    还需要设置arp_announce选项为2，设置方法同上
2、在真实服务器上添加虚拟IP
    ifconfig lo：0 192.168.10.10 boradcast 207.175.44.110 netmask 255.255.255.255
    ip r add 192.168.10.10 dev lo
接着添加ipvs规则：
添加地址为192.168.10.10:80的虚拟服务，指定调度算法为轮转。
ipvsadm -A -t 192.168.10.10:80 -s rr
添加真实服务器，指定传输模式为DR
ipvsadm -a -t 192.168.10.10:80 -r 192.168.10.1:80 -g
ipvsadm -a -t 192.168.10.10:80 -r 192.168.10.2:80 -g
ipvsadm -a -t 192.168.10.10:80 -r 192.168.10.3:80 -g
注意：此处的例子中客户、调度服务器、真实服务器都是位于同一网段的
}
    
lvs(简介)
{
    LVS是一套集成在Linux内核中的负载均衡服务。LVS通过部署负载均衡服务器在网络层截获并修改报文并依据一定规则分发给服务器集群中
服务器来实现负载均衡。LVS集群的体系图如下图所示：

它主要由三部分组成：
        负载调度器：它是集群中面向客户的服务器。它把自己伪装成客户要请求的服务器，并截获客户的报文，按照一定的规则将
报文转发至它背后的真实服务器（real server）。它拥有一个对外服务的地址Virtual Address，所有的客户都会认为这个地址就是
web服务器的唯一地址，它们会将请求发送至这个地址。负载调度器收到用户的报文后再根据调度算法将报文转发给背后的真实服务器。
真实服务器和负载调度器之间可以是通过LAN或者WAN相连。
        服务器集群：这是由若干台真正向客户提供服务的真实服务器组成的集群。它们接受并处理来自负载调度器的客户请求报文。
集群中的服务器可以动态的增减，这也是LVS的强大可伸缩性的体现。
        后端存储：为服务器集群提供一个共享的存储区。通常是通过分布式存储技术实现，让多个服务器共享一块巨大的存储空间。
        Graphic Monitor：用于监控整个集群的运行状态。

    负载调度器采用基于IP和基于内容调度结合的方式。即为每个连接分配一个服务器，属于同一个连接的数据会被分配到同一个服务器。
    LVS有着非常高的可用性，LVS会采用心跳检测（HearBeat Deamon）来检测服务器集群的状态，如果有服务器失效，LVS会立刻将该服务器
移出服务器列表并不再将数据转发至该服务器。对于负载调取服务器本身，LVS可以实现两台负载调度器形成主备，如果主调度器失效，
可以立刻切换至备调度器。

}
lvs(三种负载均衡方法)
{
    LVS的核心就在于负载调度器的实现。LVS的负载调度器主要使用IPVS（IP虚拟服务器）实现，即基于IP包对数据进行调度。LVS中主要有
三种负载均衡技术：

    1、通过NAT实现虚拟服务器（VS/NAT)
    这种情况下负载调度器收到用户的数据后，会通过NAT技术把数据包的目的地址和端口号改成某台真实服务器的地址和端口号并转发给真实服务器。
真实服务器处理请求后将响应返回给负载调度器，并由调取器进行再次NAT后发给用户。同时，当有一个请求到来后，负载调度器会在连接hash表中
记录这个连接，当这个连接的下一个报文到达后，直接从hash表中取出选好的服务器地址和端口号进行更写，以保证同一个请求的数据总是到达
同一个服务器。
    这种方案是LVS中最简单可行的一种方案，但是这种方案下所有的请求、响应数据都要经过负载调度器，这容易使负载调度器成为系统的瓶颈。
    2、通过IP隧道实现虚拟服务器（VS/TUN)
    该方案下真实服务器可以直接将响应发给客户而不必经过负载调度器，负载调取器只负责转发请求包，这就大大减轻了负载调度器的负担。
该方案下负载调度器将请求报文通过隧道封装后发给真实服务器，封装后的报文的目的地址变为真实服务器，同时内层报文包含目的地址为
Virtual Address的报头。真实服务器将报头解开处理后直接将响应的源地址设为Virtual Address（而非真实服务器自己的实际地址）并发给
客户。这样客户会以为与自己通信的一直是一台服务器。
    3、通过直接路由实现虚拟服务器(VS/DR)
    类似于TUN，这种方案下负载调度器也是只负责转发请求包。但是区别在于负载调度器不再对数据进行隧道封装，而是会直接把数据包的MAC地址
改为真实服务器的MAC地址并转发给真实服务器。但是由于是通过修改MAC地址转发，所以负载调度器必须和真实服务器集群位于同一个个网段。
    三种方法的比较如下：

    三种方法中DR的效率最高但是对环境有要求，NAT的效率最低但是实现最简单方便。TUN的效率也非常高，但是需要集群的服务器支持隧道封装技术。

}


lvs(负载调度算法)
{
1、内核中的调度算法
    IPVS在内核中的调度算法主要是以连接为粒度的。同一个连接的所有数据都会被发往同一个服务器，同一个用户的不同连接的数据则可能会
 发往不同的服务器。目前主要有以下八种调度算法：
    轮叫调度：到来每个连接轮流分配给各个服务器。即每次调度执行i = (i + 1) mod n选择第i台服务器。适合各个服务器能力均等的情况。
    加权轮叫调度：在轮叫调度的基础上对每个服务器引入权重。
    最小连接调度：把新连接分配到活跃连接数最小的服务器上。需要监控每个服务器的活跃连接数。该算法可以避免处理时间长的请求集中在一台服务器上。但是由于TCP协议2分钟TIME-WAIT状态的存在，这个算法的效率不见得理想。
    加权最小连接调度：在最小连接调度算法的基础上引入权重。
    基于局部性的最小连接：将相同目的IP的数据调度到同一服务器，通常用于cache集群（cache集群中请求的目的IP常变）
    带复制的基于局部性的最小连接：与上一个的不同在于映射到一组服务器而非一个
    目标地址散列调度：通过Hash函数将目的IP映射到一台服务器上进行服务。
    源地址散列调度：用源地址进行映射
2.动态反馈负载均衡算法
    在调度的同时考虑服务器的实时负载和响应情况，动态调整每个服务器的处理比例。
}

    IPVS的实现主要是通过Linux的Netfilter模块。IPVS通过在Netfilter模块中的LOCAL_IN和FORWARD两个钩子处挂接处理函数来截获数据包并
对数据包进行响应修改以实现调度功能.