cpu虚拟化：
	模拟：emulation
	虚拟：virtulization
		完全虚拟化（full-virtulization）
			BT: 二进制翻译 （软件）
			HVM：硬件辅助的虚拟化 （硬件）
		半虚拟化(para-virtulization)

Memory: 
	进程：线性地址空间
	内核：物理地址空间

	MMU Virtulization
		Intel: EPT, Extended Page Table
		AMD: NTP, Nested Page Table

	TLB virtulization
		tagged TLB

I/O: 
	外存：
		硬盘、光盘、U盘
	网络设备：
		网卡
	显示设备：
		VGA: frame buffer机制
	键盘鼠标：
		ps/2, usb

	I/O虚拟化的方式：
		模拟: 完全使用软件来模拟真实硬件
		半虚拟化化: 
		IO-through: IO透传

	Intel: VT-d
		基于北桥的硬件辅助的虚拟化技术；

两种实现方式：
	Type-I:
		xen, vmware ESX/ESXi
	Type-II:
		kvm, vmware workstation, virtualbox


Intel硬件辅助的虚拟化：
	CPU: vt-x, EPT, tagged-TLB
	IO/CPU: vt-d, IOV, VMDq

	第一类：跟处理器相关：vt-x
	第二类：跟芯片相关：vt-d
	第三类：跟IO相关：VMDq和SR-IOV

QEMU, virtio

虚拟化技术的分类：
	模拟：著名的模拟器，PearPC, Bochs, QEMU
	完全虚拟化：也称为native virtulization
		两种加速方式：
			BT
			HVM
		VMware Workstation, VMware Server, Parallels Desktop, KVM, Xen(HVM)
	半虚拟化：para-virtualization
		xen, uml(user-mode linux)
	OS级别的虚拟化:
		OpenVZ, lxc
		Solaris Containers
		FreeBSD jails
	库虚拟化：
		wine
	应用程序虚拟化：
		jvm

虚拟化网络：
	nat mode
	bridge mode
	routed mode
	isolation mode


使用brctl的配置的过程示例：
	# brctl addbr br0
	# brctl stp br0 on
	# ifconfig eth0 0 up
	# brctl addif br0 eth0
	# ifconfig br0 IP/NETMASK up
	# route add default gw GW


TUN与TAP
----------------------
在计算机网络中，TUN与TAP是操作系统内核中的虚拟网络设备。不同于普通靠硬件网路板卡实现的设备，这些虚拟的网络设备全部用软件实现，并向运行于操作系统上的软件提供与硬件的网络设备完全相同的功能。

TAP等同于一个以太网设备，它操作第二层数据包如以太网数据帧。TUN模拟了网络层设备，操作第三层数据包比如IP数据封包。

操作系统通过TUN/TAP设备向绑定该设备的用户空间的程序发送数据，反之，用户空间的程序也可以像操作硬件网络设备那样，通过TUN/TAP设备发送数据。在后种情况下，TUN/TAP设备向操作系统的网络栈投递（或“注入”）数据包，从而模拟从外部接受数据的过程。




Xenoserver, Ian Pratt, Keir Fraser

VMM, Xen, Type-I

Linux-2.6.24+: DomU
Linux-2.6.37+: Dom0
Linux-3.0+: 为xen做了专门优化

Xen的工作模式：
	pv: 半虚拟化
	fv: 依赖于CPU的HVM
	pv on hvm: CPU完全虚拟化，但IO采用半虚拟化；

xen的使用方式：
	xen hypervisor
		硬件 --> 安装Linux --> 安装xen --> 修改grub，配置其内核为xen，而不再使用原Linux的内核（Linux的内核和initramfs文件被当作xen的模块）--> 重新编译Linux内核，使得其能够运行于Dom0 --> 重启系统；
		DomU: 在Dom0使用工具创建虚拟机-->启动虚拟机-->安装操作系统；
	xenserver: 
		citrix: 
	xcp: xen cloud platform

	工具栈：xm/xend, xl, xapi/xe
		libvirt: virsh/libvirtd, virtmanager









Ring -1, shadow MMU, tagged TLB

IO硬件虚拟化：IOMMU


Qemu: 









虚拟化模型：
	完全虚拟化
	半虚拟化
	硬件辅助虚拟化
		pv on hvm

虚拟化的实现方式：
	Host+VMM (Hosted)
	hypervisor (bare-metal)
	emulation

虚拟化的实现种类：
	hosted
	bare-metal
	os-level (openvz)
	library 
	application

HVM:
	Intel: VT-x
	AMD: AMD-V

	MMU: 
		Intel: EPT
		AMD: NPT

	IO: IOMMU

vmware, xen, kvm, virtualbox

vmware server, vmware ESXi
vmware workstation, vmware ESX

xen, kvm

qemu

xen project
xenserver 
xcp


rhel, centos

rhel 5.3: xen
	kernel-xen, xen
	kernel
rhel 5.4: xen, kvm
	kernel-xen, xen = vmm
	kernel, kvm = vmm

rhel 6.0: xen-, kvm
	rhel6.0+ 只支持运行为DomU中的系统


Linux: 2.6.24+: 收录进来了将Linux运行Xen DomU中的代码；
Linux: 2.6.37+: 收录进来了将Linux运行Xen Dom0中的代码；
Linux: 3.0+: 收录进来对Xen优化种的各种驱动；

32bits: 4G，PAE（64G）
64bits: 

KVM：KVM只能运行在支持硬件虚拟化的CPU上，并且只支持64bits系统；

4.0: xm/xend
4.1: xl, xm/xend
4.2: xl

仅运行xen虚拟化：
qemu: 


virsh/libvirtd


centos: kernel-3.4
xen: xen-4.1.3
libvirt: 


































1974年，Popek和Goldberg在一篇论文中定义了“经典虚拟化(Classical virtualization)”的基本需求，他们认为，一款真正意义上的VMM至少要符合三个方面的标准：

◇ 等价执行(Equivalient execution)：除了资源的可用性及时间上的不同之外，程序在虚拟化环境中及真实环境中的执行是完全相同的；
◇ 性能(Performance)：指令集中的大部分指令要能够直接运行于CPU上；
◇ 安全(Safety)：VMM要能够完全控制系统资源；


1.1 x86平台实现虚拟化技术的挑战

x86处理器有4个特权级别，Ring 0 ~ Ring 3，只有运行在Ring 0 ~ 2 级时，处理器才可以访问特权资源或执行特权指令；运行在 Ring 0级时，处理器可以运行所有的特权指令。x86平台上的操作系统一般只使用Ring 0和Ring 3这两个级别，其中，操作系统运行在Ring 0级，用户进程运行在 Ring 3 级。
 

1.1.1 特权级压缩(ring compression)

为了满足上面所述的需求，VMM自身必须运行在Ring 0级，同时为了避免GuestOS控制系统资源，GuestOS不得不降低自身的运行级别而运行于Ring 3（Ring 1、2 不使用）。

此外，VMM使用分页或段限制的方式保护物理内存的访问，但是64位模式下段限制不起作用，而分页又不区分Ring 0,1,2。为了统一和简化VMM的设计，GuestOS只能和用户进程一样运行在 Ring 3。VMM必须监视GuestOS对GDT、IDT等特权资源的设置，防止GuestOS运行在Ring 0级，同时又要保护降级后的GuestOS不受Guest进程的主动攻击或无意破坏。

1.1.2 特权级别名（Ring Alias）

设计上的原因，操作系统假设自己运行于ring 0，然而虚拟化环境中的GuestOS实际上运行于Ring 1或Ring 3，由此，VMM必须保证各GuestOS不能得知其正运行于虚拟机中这一事实，以免其打破前面的“等价执行”标准。例如，x86处理器的特权级别存放在CS代码段寄存器内，GuestOS却可以使用非特权PUSH指令将CS寄存器压栈，然后POP出来检查该值；又如，GuestOS在低特权级别时读取特权寄存器GDT、LDT、IDT和TR时并不发生异常。这些行为都不同于GuestOS的正常期望。

1.1.3 地址空间压缩（Address Space Compression）

地址空间压缩是指VMM必须在GuestOS的地址空间中保留一段供自己使用，这是x86虚拟化技术面临的另一个挑战。VMM可以完全运行于自有的地址空间，也可以部分地运行于GuestOS的地址空间。前一种方式，需在VMM模式与GuestOS模式之间切换，这会带来较大的开销；此外，尽管运行于自己的地址空间，VMM仍需要在GuestOS的地址空间保留出一部分来保存控制结构，如IDT和GDT。无论是哪一种方式，VMM必须保证自己用到地址空间不会受到GuestOS的访问或修改。

1.1.4 非特权敏感指令

x86使用的敏感指令并不完全隶属于特权指令集，VMM将无法正确捕获此类指令并作出处理。例如，非特权指令SMSW在寄存器中存储的机器状态就能够被GuestOS所读取，这违反了经典虚拟化理论的要求。

1.1.5 静默特权失败(Silent Privilege Failures)

x86的某些特权指令在失败时并不返回错误，因此，其错误将无法被VMM捕获，这将导致其违反经典虚拟化信条中的“等价执行”法则。

1.1.6 中断虚拟化(Interrupt Virtualization)

虚拟化环境中，屏蔽中断及非屏蔽中断的管理都应该由VMM进行；然而，GuestOS对特权资源的每次访问都会触发处理器异常，这必然会频繁屏蔽或启用中断，如果这些请求均由VMM处理，势必会极大影响整体系统性能。

1.2  X86平台虚拟化

完整意义上的计算机由硬件平台和软件平台共同组成。根据计算机体系结构理论，其硬件平台包括CPU、内存和各种I/O设备；而软件平台则包括BIOS、操作系统、运行时库及各种应用程序等。对于主机虚拟化技术来讲，其主要负责虚拟硬件平台及BIOS，而操作系统、运行时库及各种应用程序可以使用以往在物理平台上各种现有技术及产品。




网络模型：



x86: 
	CPU: 
		Intel VT-x
		AMD AMD-V

	MMU:
		Intel EPT
		AMD   NPT

	IO:
		Intel IOMMU



emulated: Qemu

OS + VMM

bare-metal: VMM + Guest

虚拟化种类：
	Full Virtualization
	Para Virtualization
		GuestOS的内核了解自己工作在VMM之上；
	Emulator
	OS-Level, (Container)
		OpenVZ, UML
	Library Virtualization
		Wine, cywin
	Application Virtulization

Xen, KVM

Xen


RHEL5.3, Xen
RHEL5.4: Xen 和 KVM(64bits)

RHEL6.0: KVM(64bits)


RHEL6.0:　DomU, 但不能运行为Dom0


Linux: 2.6.24+ pvops framework
	DomU
Linux: 2.6.37 (3.0+)
	Dom0


RHEL 6.4 64bits:
	1、
	

"Development Tools" "Server Platform Development" "Desktop Platform Development" ""

Xen



kernel /xen.gz
module /
module /

Xen-4.0
	xend/xm


Xen-4.1 
	xl, xend/xm
	

Xen-4.2
	xl


Xen: 虚拟机平台
	xm/xl

Qemu: qemu 

libvirt
	libvirtd/virsh
	virt-manager: GUI
	virt-install: 

IaaS: 

OpenStack
CloudStack
OpenNebular




KVM/Qemu --> Virsh , qemu-kvm
















Dom0: 2.6.37

关键性驱动及优化：3.0

PV, HVM, PV on HVM

Linux pvops framework: 2.6.24




1.4.2 常见的工具栈

◇ Default / XEND

Xen 4.0及之前的版本中默认使用的工具栈，Xen 4.1提供了新的轻量级工具栈xl，但仍然保留了对Xend/xm的支持，但Xen 4.2及之后的版本已弃用。但xl在很大程度上保持了与xm的兼容。

◇ Default / XL

xl是基于libxenlight创建的轻量级命令行工具栈，并从Xen 4.1起成为默认的工具栈。xl与Xend的功能对比请参照http://wiki.xen.org/wiki/XL_vs_Xend_Feature_Comparison。

◇ XAPI / XE

XAPI即Xen管理API(The Xen management API)，它是Citrix XenServer和XCP默认使用的工具栈。目前，其移植向libxenlight的工作正进行中。XAPI是目前功能最通用且功能最完备的Xen工具栈，CloudStack、OpenNebula和OpenStack等云计算解决方案都基于此API管理Xen虚拟机。






一、配置网络接口

[root@el6 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE="eth0"
HWADDR="00:11:22:33:44:55"
NM_CONTROLLED="no"
ONBOOT="yes"
BOOTPROTO="dhcp"

最重要的是确保NM_CONTROLLED的值为"no"，以及ONBOOT的值为"yes"。

另外，要确保网络服务能够开机自动启动：
# chkconfig network on

在/etc/hosts文件中为本机的主机名及IP地址建立解析
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.7	virtlab.magedu.com	virtlab

禁用SELinux。

二、解决依赖关系


# yum install screen vim wget tcpdump ntp ntpdate man smartmontools links lynx ethtool xorg-x11-xauth


修改grub.conf，增加超时时间，并禁用hiddenmenu。

#boot=/dev/sda
default=0
timeout=15
splashimage=(hd0,0)/grub/splash.xpm.gz
#hiddenmenu
title Red Hat Enterprise Linux (2.6.32-279.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-279.el6.x86_64 ro root=/dev/mapper/vg0-root rd_NO_LUKS LANG=en_US.UTF-8 rd_LVM_LV=vg0/swap rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto rd_NO_DM  KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=vg0/root rhgb quiet
        initrd /initramfs-2.6.32-279.el6.x86_64.img


三、安装编译Xen所依赖的工具

# yum groupinstall "Development tools" "Additional Development" "Debugging Tools" "System administration tools" "Compatibility libraries" "Console internet tools" "Desktop Platform Development"


# yum install transfig wget texi2html libaio-devel dev86 glibc-devel e2fsprogs-devel gitk mkinitrd iasl xz-devel bzip2-devel pciutils-libs pciutils-devel SDL-devel libX11-devel gtk2-devel bridge-utils PyXML qemu-common qemu-img mercurial texinfo libuuid-devel ocaml ocaml-findlib

# yum install glibc-devel.i686


四、Rebuilding and installing Xen src.rpm

# cd /usr/local/src
# wget http://xenbits.xen.org/people/mayoung/EL6.xen/SRPMS/xen-4.1.2-20.el6.src.rpm
# rpm -ivh xen-4.1.2-20.el6.src.rpm
# cd /root/rpmbuild/SPECS/
# rpmbuild -bb xen.specs

五、安装qemu(此步非必须)

# yum install usbredir-devel spice-protocol spice-server-devel libseccomp-devel systemtap-sdt-devel nss-devel xfsprogs-devel bluez-libs-devel brlapi-devel libcap-devel
# wgethttp://mirrors.ustc.edu.cn/fedora/linux/releases/15/Everything/source/SRPMS/qemu-0.14.0-7.fc15.src.rpm
# rpm -ivh qemu-1.2.0-23.fc18.src.rpm
# cd /root/rpmbuild/SPECS
# rpmbuild -bb qemu.spec


六、编译内核

#
# 


# These core options (Processor type and features| Paravirtualized guest support]
CONFIG_PARAVIRT=y
CONFIG_XEN=y
CONFIG_PARAVIRT_GUEST=y
CONFIG_PARAVIRT_SPINLOCKS=y
# add this item

# And Xen pv console device support (Device Drivers|Character devices
CONFIG_HVC_DRIVER=y
CONFIG_HVC_XEN=y

# And Xen disk and network support (Device Drivers|Block devices and Device Drivers|Network device support)
CONFIG_XEN_FBDEV_FRONTEND=y
CONFIG_XEN_BLKDEV_FRONTEND=y
# change the value to y
CONFIG_XEN_NETDEV_FRONTEND=y
# change the value to y

# And the rest (Device Drivers|Xen driver support)
CONFIG_XEN_PCIDEV_FRONTEND=y
CONFIG_INPUT_XEN_KBDDEV_FRONTEND=y
CONFIG_XEN_FBDEV_FRONTEND=y
CONFIG_XEN_XENBUS_FRONTEND=y
CONFIG_XEN_SAVE_RESTORE=y
CONFIG_XEN_GRANT_DEV_ALLOC=m

# And for tmem support:
CONFIG_XEN_TMEM=y
# add the item
CONFIG_CLEANCACHE=y
# enable the item
CONFIG_FRONTSWAP=y
# enable the item
CONFIG_XEN_SELFBALLOONING=y
# add the item

# Configure kernel for dom0 support
# NOTE: Xen dom0 support depends on ACPI support. Make sure you enable ACPI support or you won't see Dom0 options at all.
# In addition to the config options above you also need to enable:
CONFIG_X86_IO_APIC=y
CONFIG_ACPI=y
CONFIG_ACPI_PROCFS=y (optional)
CONFIG_XEN_DOM0=y
CONFIG_PCI_XEN=y
CONFIG_XEN_DEV_EVTCHN=y
CONFIG_XENFS=y
# change the value to y
CONFIG_XEN_COMPAT_XENFS=y
CONFIG_XEN_SYS_HYPERVISOR=y
CONFIG_XEN_GNTDEV=y
CONFIG_XEN_BACKEND=y
CONFIG_XEN_NETDEV_BACKEND=m
# enable the item
CONFIG_XEN_BLKDEV_BACKEND=m
# enable the item
CONFIG_XEN_PCIDEV_BACKEND=m
CONFIG_XEN_PRIVILEGED_GUEST=y
CONFIG_XEN_BALLOON=y
CONFIG_XEN_SCRUB_PAGES=y

# If you're using RHEL5 or CentOS5 as a dom0 (ie. you have old udev version), make sure you enable the following options as well:
CONFIG_SYSFS_DEPRECATED=y
CONFIG_SYSFS_DEPRECATED_V2=y


七、编辑安装libvirt

yum install libblkid-devel augeas sanlock-devel radvd ebtables systemtap-sdt-devel scrub numad

# cd /usr/src
# wget ftp://ftp.redhat.com/pub/redhat/linux/enterprise/6Server/en/os/SRPMS/libvirt-0.9.10-21.el6_3.7.src.rpm
# rpm -ivh libvirt-0.9.10-21.el6_3.7.src.rpm
编辑/root/rpmbuild/SPECS/libvirt.spec文件，启用with_xen选项。

# cd /root/rpmbuild/SPECS

# rpmbuild -bb libvirt.spec


yum install libnl-devel xhtml1-dtds libudev-devel libpciaccess-devel yajl-devel libpcap-devel avahi-devel parted-devel device-mapper-devel numactl-devel netcf-devel xen-devel dnsmasq iscsi-initiator-utils gtk-vnc-python

先安装gtk-vnc-python包。

编译安装virtinst

编译安装libvirtd

编译安装virt-manager




注意：libvirt-1.0之前的版本不支持xen 4.2。


/proc/xen/

Xen: para virt
	创建DomU, 并安装OS, 只能基于网络安装；
		NFS, FTP, HTTP





[xen4]
name=xen4
baseurl=ftp://172.16.0.1/pub/Sources/6.x86_64/virtualization_bak/xen4
gpgcheck=0
cost=500





kernel /xen.gz dom0_mem=1024M,max:1024M loglvl=all guest_loglvl=all


grub配置：

#boot=/dev/sda
default=0
timeout=5
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Red Hat Enterprise Linux Server (3.7.4-1.el6xen.x86_64)
        root (hd0,0)
        kernel /xen.gz dom0_mem=1024M cpufreq=xen dom0_max_vcpus=2 dom0_vcpus_pin
        module /vmlinuz-3.7.4-1.el6xen.x86_64 ro root=/dev/mapper/vg0-root rd_NO_LUKS LANG=en_US.UTF-8 rd_LVM_LV=vg0/swap rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto rd_NO_DM  KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=vg0/root rhgb quiet
        module /initramfs-3.7.4-1.el6xen.x86_64.img
title Red Hat Enterprise Linux (2.6.32-279.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-279.el6.x86_64 ro root=/dev/mapper/vg0-root rd_NO_LUKS LANG=en_US.UTF-8 rd_LVM_LV=vg0/swap rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto rd_NO_DM  KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=vg0/root rhgb quiet
        initrd /initramfs-2.6.32-279.el6.x86_64.img


xen虚拟状态：
	r: 
	b: 阻塞
	p: 暂停
	s: 停止
	c: 崩溃
	d: dying, 正在关闭的过程中

如何创建xen pv模式：
	1、kernel
	2、initrd或initramfs
	3、DomU内核模块
	4、根文件系统
	5、swap设备
	6、DomU的配置文件
	
xm的配置文件：
	kernel：内核
	ramdisk: initramfs或initrd
	name: 域名称
	memory: 内存大小
	disk: 磁盘设备文件列表，格式disk=["disk1", "disk2",], 每个disk都由三个参数进行定义：“backend-dev”，“frontend-dev”，“mode”
		backend-dev: 有两种类型，物理设备，虚拟磁盘映像文件，格式为分别为phy:device和file:/path/to/image_file; 
		front-dev: 定义其在DomU中设备类型；虚拟磁盘映像文件对应的设备文件名称通常为xvd[a-z]
		mode: 访问权限模型，r, w
	vcpus: 虚拟CPU的个数；
	root: 根文件系统所在的设备；
	extra: 传递给内核的额外参数；selinux=0
	on_reboot: 执行xm reboot命令时的操作，有destroy和restart; 
	on_crash: 有destroy, restart, preserve(保存崩溃时的信息以用于调试)
	vif ：vif = ['ip="172.16.100.11", bridge=br0']
		type: 设备类型，默认为netfront
		mac: 指定mac地址；
		bridge: 指定桥接到的物理设备
		ip: ip地址；
		script: 配置此接口的脚本文件
		vifname: 后端设备名称
	bootloader: 引导器文件的路径，一般指的PyGrub的路径；
	







Dom1: 


  xm create /dev/null ramdisk=initrd.img \
                    kernel=/boot/vmlinuz-2.6.12.6-xenU \
                    name=ramdisk vif='' vcpus=1 \
                    memory=64 root=








./configure \
  --prefix=/usr \
  --sbin-path=/usr/sbin/nginx \
  --conf-path=/etc/nginx/nginx.conf \
  --error-log-path=/var/log/nginx/error.log \
  --http-log-path=/var/log/nginx/access.log \
  --pid-path=/var/run/nginx/nginx.pid  \
  --lock-path=/var/lock/nginx.lock \
  --user=nginx \
  --group=nginx \
  --with-http_stub_status_module 























接功能是在内核中实现的，因此，只要提供了正确网络接口的配置文件，Linux系统会自动启用桥接设备。比如，如果想为当前系统定义桥接设备xenbr0，并将eth0设备附加其上，在rhel6平台上，其步骤如下。

假设eth0的原始配置信息如下面所示。
DEVICE="eth0"
BOOTPROTO="static"
NM_CONTROLLED="no"
HWADDR=00:0C:29:B4:48:BB
IPADDR=192.168.1.7
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
ONBOOT="yes"
TYPE="Ethernet"

为xenbr0创建配置文件/etc/sysconfig/network-scripts/ifcfg-xenbr0，其内容类似下面所示。需要注间的是TYPE属性的值Bridge一定要大写首字母。
DEVICE="xenbr0"
BOOTPROTO="static"
NM_CONTROLLED="no"
IPADDR=192.168.1.7
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
ONBOOT="yes"
TYPE=" Bridge"
DELAY=0

将eth0的配置文件/etc/sysconfig/network-scripts/ifcfg-eth0的内容修改为：
DEVICE="eth0"
BOOTPROTO="static"
NM_CONTROLLED="no"
HWADDR=00:0C:29:B4:48:BB
ONBOOT="yes"
TYPE="Ethernet"
BRIDGE=xenbr0

而后使用/etc/init.d/network脚本重启网络服务即可。

6.1.1.3 使用brctl命令管理桥接设备

brctl是由bridge-utils软件包提供的命令，使用前请先确保其已经正确安装。brctl有许多的子命令，分别用于实现添加、删除桥接设备，向桥设备附加网络接口等。不带任何参数的brctl命令即可显示其使用帮助，如下列出了其中的一部分。
# brctl
Usage: brctl [commands]
commands:
	addbr     	<bridge>		add bridge
	delbr     	<bridge>		delete bridge
	addif     	<bridge> <device>	add interface to bridge
	delif     	<bridge> <device>	delete interface from bridge
	show      				show a list of bridges
	…… ……

由上面的帮助信息所示，添加一个桥接设备只需使用命令“brctl  addbr  <bridge_name>”即可，比如，添加xenbr0设备：
# brctl  addbr  xenbr0

刚添加桥接设备时，由于没有为其附加任何网络接口，其尚处于未激活状态。为其附加一个网络接口设备可使用“brctl  addif  <bridge_name>  <device_name>命令实现，比如，将eth0附加至刚添加的xenbr0：
# brctl  addif  xenbr0  eth0

从桥设备上删除某网络接口，使用命令“brctl  delif  <bridge_name>  <device_name>”；删除桥接设备则使用“brctl  delbr  <bridge_name>”，不过需要注意的是，只有在桥设备处于未激活状态时才能将其删除。而要查看桥接设备相关的信息，可以使用“brctl show”命令。

需要注意的是，使用brctl命令配置桥接设备的完整过程需要参考前面讲解的network-bridge start命令的实现步骤进行。


















# To create one using the VNC backend and sensible defaults:
#
# vfb = [ 'type=vnc' ]
#
# The backend listens on 127.0.0.1 port 5900+N by default, where N is
# the domain ID.  You can override both address and N:
#
# vfb = [ 'type=vnc,vnclisten=127.0.0.1,vncdisplay=1' ]
#
# Or you can bind the first unused port above 5900:
#
# vfb = [ 'type=vnc,vnclisten=0.0.0.0,vncunused=1' ]
#
# You can override the password:
#
# vfb = [ 'type=vnc,vncpasswd=MYPASSWD' ]
#
# Empty password disables authentication.  Defaults to the vncpasswd
# configured in xend-config.sxp.

vfb = [ 'type=vnc,vncdisplay=10,vncpasswd=s3cr3t' ]



DomU:
	1、根文件系统；
	2、kernel及ramdisk；
		Dom0：外部
		DomU: 内部 

xm
	create
	destroy
	shutdown
	console
	list
	network-attach
	network-detach
	block-attach
	block-detach

	delete：删除虚拟机

	pause: 暂停
	unpause: 从暂停中恢复

	suspend: 挂起
	resume: 从挂起中恢复；


	save: 保存状态至文件中
	restore: 从保存状态中恢复

	top: 资源使用状态监控

	info: 查看主机相关信息，如内存等；




	virt-manager GUI

	虚拟机的配置文件：/etc/xen




批量部署DomU:
	准备一个映像模板；
		OZ：
	脚本：
		生成一个配置文件/etc/xen
		下载一个磁盘映像

16：


disk = ['phy:/dev/sdb,xvda,w']


使用了bootloader, pygrup示例：
#ramdisk="/boot/initramfs-2.6.32-358.el6.x86_64.img"
#kernel="/boot/vmlinuz-2.6.32-358.el6.x86_64"
name="linux"
vcpus=1
memory=128
disk=['file:/xen/vm2/dom2.img,xvda,w',]
bootloader="/usr/bin/pygrub"
#root="/dev/xvda2 ro"
#extra="selinux=0 init=/sbin/init"
vif=[ 'bridge=br0' ]
on_crash="destroy"
on_reboot="restart"


使用Dom0中的kernel和ramdisk引导的示例：
ramdisk="/boot/initramfs-2.6.32-358.el6.x86_64.img"
kernel="/boot/vmlinuz-2.6.32-358.el6.x86_64"
name="test"
vcpus=1
memory=128
disk=['file:/xen/vm1/test.img,xvda,w',]
root="/dev/xvda ro"
extra="selinux=0 init=/sbin/init"



自定义安装，并启用了vnc功能：
#ramdisk="/xen/isolinux/initrd.img"
#kernel="/xen/isolinux/vmlinuz"
name="rhel6"
vcpus=2
memory=512
disk=['file:/xen/vm3/rhel6.img,xvda,w',]
bootloader="/usr/bin/pygrub"
#root="/dev/xvda2 ro"
#extra="selinux=0 init=/sbin/init"
#extra="ks=http://172.16.0.1/rhel6.x86_64.cfg"
vif=[ 'bridge=br0' ]
on_crash="destroy"
on_reboot="destroy"
vfb=[ 'vnc=1,vnclisten=0.0.0.0' ]



xen: 
	1、type-i: xen hypervisor
	2、Dom0: 特权域，管理IO
	3、管理控制台：Dom0
		xm/xend, xl
	# xm create 

	pv: 
		启动一个虚拟机实例，内核和initramfs文件可以放置在Dom0, 也可由某DomU自己的文件系统提供：
			第一种：配置文件，无需bootloader，由xm等管理工具
				kernel=
				ramdisk=
				root=
				extra=
			第二种：需要一个bootloader，pygrub
				bootloader=

	xm基于某过程引导安装过程：
		isolinux:
			vmlinuz
			initrd.img

























创建Xen PV模式虚拟机的前提

在PV模式中运行guest系统，需要满足几个基本前提。
◇	能运行于Xen DomU的(Xen-enabled)内核：Liunx 2.6.24及以后的内核已经添加了对Xen DomU的支持，因此，只要在内核编译时启用了相应的功能就能满足此要求，目前多数Linux发行版的内核都已经支持此特性；而此前的版本需要内核在编译前手动打补丁方可；
◇	根文件系统(Root Filesystem)：包含了应用程序、系统组件及配置文件等运行DomU的各种所需要文件的文件系统，其不用非得包含内核及对应的ramdisk，后面的这些组件放在Dom0中即可；事实上，用于DomU的内核文件必须要能够允许Dom0访问到，因为其运行时需要与Xen Hypervisor通信，因此，这些内核组件可以位于Dom0能够访问到的任何文件系统上；然而，目前基于pygrub(可用于Dom0跟非特权域磁盘映像中的内核通信)，此内核文件也可以直接放置于非特权域的磁盘映像中；
◇	DomU内核所需要的内核模块：内核模块是内核的重要组成部分，它们一般存储于根文件系统；
◇	ramdisk或者ramfs：这个根据实际需要是个可选组件，如果在内核初始化过程中不需要依赖于此来装载额外的驱动程序以访问根文件系统则也可以不用提供；
◇	swap设备：交换分区能够让Linux运行比仅有物理内存时更多的进程，因此，提供此组件是常见的做法；当然，它是可选的；
◇	DomU配置文件：集中在一起指定前述各组件的配置信息，以及定义其它有关PV DomU的基本属性的文件；其通常包含所有用于当前DomU属性配置参数，包括为其指定磁盘映像和内核文件的位置(或pygrub的位置)等，以及其它许多属性如当前DomU可以访问的设备等，这些设备包括网络设备、硬盘、显卡及其它PCI设备；同时，配置文件中也可以指定新创建的非特权域可以使用的物理内存大小及虚拟CPU个数等等；

这里需要提醒的是，如果计划为PV DomU编译内核，需要以与传统方式不同的方式放置内核及其模块。前面也已经提到，内核一般会放在Dom0的某路径下，而内核模块则需要放在DomU的根文件系统上。

PV DomU的根文件系统可以以多种不同的方式进行安置，比如：
◇	虚拟磁盘映像文件
◇	Dom0没有使用的额外物理磁盘分区
◇	Dom0没有使用的逻辑卷
◇	块级别网络文件系统，如iSCSI设备
◇	网络文件系统，如NFS

有许多组织提供了预配置的根文件系统，如FreeOsZoo(www.oszoo.org)、Jailtime.org(www.jailtime.org)等，读者可以根据需要到它们的站点下载。另外，rPath还提供了一个在的根文件系统制作系统rBuilder(www.rpath.com/rbuiler)。

Oz


xm create 
	disk=
		rootfs

仅有xm工具：如何安装?
	实现规模化部署多个虚拟机实例：
		vif=['mac=']














4.2 案例1：基于Dom0(rhel6.4 x86_64)制作一个运行于DomU的微型Linux系统

xm create命令能够基于预配置的根文件系统、能运行于Xen DomU的内核、ramfs和Xen配置文件启动DomU，当然，在其启动之前，所有需要用到的其它组件都需要准备就绪方才可以。这一切准备就绪后，使用xm create命令有点类似于按下主机的电源按钮对其进行启动。下面开始制作过程。

4.2.1 制作根文件系统

创建空的虚拟磁盘映像，以之作为新建非特权域的虚拟磁盘映像文件，此映像文件并不真正占用为其指定的空间，而是随着存储的内容而变化。文件的路径和其虚拟空间大小(这里为2048)也可根据需要进行指定。
# dd if=/dev/zero of=/xen/test/rhel6.img oflag=direct bs=1M seek=2047 count=1

接下来将此虚拟磁盘映像格式化为ext4文件系统。
# mkfs.ext4  /xen/test/rhel6.img

这里也可以使用其它文件系统，但要确保用于DomU的内核支持此文件系统。此外，如果指定的文件系统是以内核模块的形式存在，那么还需要将对应的内核模块复制到后面制作的根文件系统上相应的路径下。对于这里实验中的Dom0来说，其内核将ext4文件系统直接编译进了内核，我们也将此内核用于后面要启动的DomU。

将格式化完毕的虚拟磁盘映像文件挂载至某目录下，如/mnt，并创建根文件系统目录结构。
# mount  -o loop  /xen/test/rhel6.img  /mnt
# mkdir  -pv /mnt/{etc/{init,rc.d},bin,sbin,lib64,dev,tmp,proc,sys}

Linux系统的启动过程中，内核初始化完成后，会运行/sbin/init以启动PID号为1的init进程，并在其配置文件的辅助下启动完成挂载额外文件系统、启动服务、启动终端等后续任务。我们这里将/sbin/init的任务精简为仅启动bash进程并打印命令提示符于终端。

# cp  /sbin/init  /mnt/sbin
# cp  /bin/bash  /mnt/bin

这两个二进制程序的运行依赖于一些系统库文件，ldd命令可以查看每个二进制程序所依赖的库文件，将每个二进制程序所依赖的库文件复制到虚拟磁盘映像文件挂载点(/mnt)中对应的路径下即可。这里以/sbin/init为例进行说明。
# ldd /sbin/init
	linux-vdso.so.1 =>  (0x00007fffddbee000)
	libnih.so.1 => /lib64/libnih.so.1 (0x00007f77027bc000)
	libnih-dbus.so.1 => /lib64/libnih-dbus.so.1 (0x00007f77025b2000)
	libdbus-1.so.3 => /lib64/libdbus-1.so.3 (0x00007f7702370000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f7702153000)
	librt.so.1 => /lib64/librt.so.1 (0x00007f7701f4b000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f7701d34000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f77019a2000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003623800000)
上面列表中的第一个库文件可以忽略。于是，我们复制/lib64/libnih.so.1到/mnt/lib64目录中即可，后面的各库文件都照此法进行。

而后查看/bin/bash所依赖的库文件，并逐个复制。在/sbin/init中已经复制过的库文件就可以略过了。如果还想让目标系统能够执行其它命令，照此法进行即可。

/sbin/init的运行还需要一些配置文件，它们是位于/etc/init目录以.conf结尾的文件。我们这里为其建立一个简单的配置文件rcS.conf。因此，这里需要创建/mnt/etc/init/rcS.conf文件，内容如下。
# rcS - runlevel compatibility
#
# This task runs the old sysv-rc startup scripts.

start on startup

stop on runlevel

task
console output
exec /etc/rc.d/rc.sysinit
# end of rcS.conf

上面的配置文件依赖于/etc/rc.d/rc.sysinit脚本来完成系统的初始化，因此，这里编辑/mnt/etc/rc.d/rc.sysinit，为其添加如下内容。
#!/bin/bash
echo -e "\tWelcome to Xen DomU test"
/bin/bash

而后执行如下命令让其具有执行权限：
# chmod +x  /mnt/etc/rc.d/rc.sysinit

完成后卸载此磁盘映像文件。
# umount  /mnt

4.2.2 为目标DomU制作配置文件

Xen PV DomU运行依赖的前提前文已经进行了详细解释。根据需要，我们这里已经制作完成一个根文件系统，接着还需要能运行于DomU的内核，相应的ramfs和Xen配置文件。事实上，这里的Dom0中使用的内核/boot/ vmlinuz-3.7.4-1.el6xen.x86_64就可以运行于DomU，其相应的ramfs文件/boot/ initramfs-3.7.4-1.el6xen.x86_64.img也可以直接使用，因此，此两者文件我们就不再另外制作。

接下来只需再为目标DomU提供一个配置文件，其就能启动了。于是我们去新配置文件/etc/xen/test，其内容如下。

kernel = "/boot/vmlinuz-3.7.4-1.el6xen.x86_64"
ramdisk = "/boot/initramfs-3.7.4-1.el6xen.x86_64.img"
name = "test"
memory = "128"
disk = [ 'file:/xen/test/rhel6.img,xvda,w', ]
vcpus=1
on_reboot = 'restart'
on_crash = 'destroy'
root = "/dev/xvda ro"
extra = "selinux=0 init=/sbin/init"





Xen支持的虚假化：
	full virt: CPU HVM, qemu emulation
		pv on hvm
	para virt

VMM, Hypervisor
	bare-metal
	Dom0: ID, Dom配置文件
		Linux-2.6.37
	DomU: Linux-2.6.24

Linux: 长期维护, 长期支持

Xen的管理接口：API
	xl
	xm/xend
	xe/xapi
	virsh/libvirt/virt-manager/virt-viewer/virtinst

IaaS: Infrastructure As A Service
	Openstack






Xen的配置文件说明


Xen配置文件一般由选项（options）、变量(variables)、CPU、网络、PCI、HVM、计时器(timers)、驱动(drivers)、磁盘设备(disk devices)、动作(behavior)，以及图形及声音(Graphics and audio)几个段组成，分别用于定义不同类别的域属性或设备属性。

上面的配置文件中的各选项作用如下。
◇	kernel：为当前域指定可用于DomU的内核文件；
◇	ramdisk：与kernel指定的内核文件匹配使用的ramdisk映像文件，根据需要指定，此为可选项；
◇	name：当前域的独有名称；每个域必须使用全局惟一的名称，否则将产生错误；
◇	memory：当前域的可用物理内存空间大小，单位为MB，默认为128；
◇	disk：当前域的所有可用磁盘设备列表，格式为disk = [ “disk1”, “disk2”, …]，每个disk都有三个参数进行定义，格式为“backend-dev，front-dev，mode”；
	backend-dev主要有两种类型，物理设备或虚拟磁盘映像文件，它们的格式分别为“phy:device”和“file:/path/to/file”；
	frontend-dev定义其在DomU中的设备类型，一般为xvd[a-z];
	mode则用于定义其访问权限，r为只读，w为读写；
◇	vcpus：配置给当前域使用的虚拟CPU的个数；默认为1；
◇	root：为当前域指定其根文件系统所在的设备，这个将作为内核参数在内核启动传递给内核；
◇	extra：传递给内核的额外参数，其中selinux=0表示禁用selinux，init则用于指定init程序的路径；多个参数之间使用空格隔开；
◇	on_reboot：执行xm reboot命令或在当前域内部执行重启操作时由Xen执行的动作；其常用的值为destroy和restart；
◇	on_crash：当前域由于各种原因崩溃时由Xen执行的动作；其常用的值为destroy、restart和preserve，preserve可以保存系统崩溃前的状态信息以用于调试；
◇	on_shutdown：执行xm shutdown命令或在当前域内部执行关机操作时由Xen执行的动作；


/dev/sdb

disk = [ 'phy:/dev/sdb,xvda,w', ]



其它常用参数：
◇	vif：定义当前域的可用虚拟网络接口列表，每个虚拟网络接口都可以使用“name=value”的格式定义其属性；也可在定义某接口时不指定任何属性，其所有属性将均由系统默认配置；例如：vif = ['ip = "192.168.1.19", bridge=xenbr0']
	type：接口设备的类型，默认为netfront；
	mac：MAC地址，默认为随机；
	bridge：桥接到的物理设备，默认为Dom0中的第一个桥接设备；
	ip：ip地址；
	script：配置此接口的脚本文件，省略时将使用默认的配置脚本；
	vifname：后端设备的设备名称，默认为vifD.N，其中D为当前域的ID，N为此网络接口的ID；
◇	vfb：为当前域定义虚拟帧缓冲，其有许多可用属性，可以使用“name=value”的格式进行定义；
	vnc或sdl：定义vnc的类型，vnc=1表示启动一个可由外部设备连接的vnc服务器，sdl=1则表示启用一个自有的vncviewer；两者可以同时使用；
	vncdisplay：vnc显示号，默认为当前域的ID，当前域的VNC服务器将监听5900+此显示号的端口；
	vnclisten：VNC服务器监听的地址，默认为127.0.0.1；
	vncunused：如果此属性的值为非零值，则表示vncserver监听大于5900的第一个没被占用的端口；
	vncpasswd：指定VNC服务器的认证密码；
	display：用于域的自有vncviewer显示，默认为DISPLAY环境变量的值；
◇	cpu：指定当前域应该在哪个物理CPU上启动，0表示第一颗CPU，1表示第二颗，依次类推；默认为-1，表示Xen可自行决定启动当前域的CPU；
◇	cpus：指定当前域的VCPU可以在哪些物理CPU上运行，如cpus = ”3,5-8,^6”表示当前域的VCPU可以在3，5，7，8号CPU上运行；
◇	bootloader：bootloader程序的路径；基于此bootloader，PV DomU的内核也可直接位于其文件系统上而非Dom0的文件系统；

更多的选项请参见xmdomain.cfg的手册页，或参考Xen官方wiki链接http://wiki.xen.org/wiki/XenConfigurationFileOptions中的详细解释。

4.2.3 启动名为test的DomU  

启动DomU，可使用xm create命令或xl create命令，但要使用xm命令，需要事先启动Xend服务；然而，要使用xl命令，则不能启动Xend服务。我们这里仍然以xm为例。
# xm create  -c  test

其中的-c选项表示启动后直接连接至虚拟机的终端，整个启动过程就在当前屏幕上显示。如下面所示的启动过程的最后几行
dracut: Switching root
	Welcome to Xen DomU test
bash: no job control in this shell
bash-4.1#

在Dom0的终端上执行xm list命令即可查看test的运行状态。
# xm list
Name                                     ID   Mem VCPUs      State   Time(s)
Domain-0                                   0   512     1     r-----    197.7
test                            


4.3案例2：基于isolinux的内核和initrd文件在PV DomU模式安装rhel 6.3

第一步，创建空的虚拟磁盘映像，以之作为新建非特权域的虚拟磁盘文件，此映像文件并不真正占用为其指定的空间，而是随着存储的内容而变化。
# dd if=/dev/zero of=/xen/vm1/rhel6.img oflag=direct bs=1M seek=40960 count=1

第二步，获取要安装的rhel6(x86_64)的isolinux目录中的vmlinuz和initrd.img文件，这里将其存放于/xen/rhel6/isolinux目录中。

第三步，为新的非特权域创建配置文件/etc/xen/rhel6，内容如下。

kernel = "/xen/rhel6/isolinux/vmlinuz"
ramdisk = "/xen/rhel6/isolinux/initrd.img"
name = "rhel6"
memory = "512"
disk = [ 'file:/xen/vm1/rhel6.img,xvda,w', ]
vif = [ 'bridge=xenbr0', ]
 
#bootloader="/usr/bin/pygrub"
#extra = "text ks=http://some_server/path/to/kickstart.cfg"
vcpus=1
on_reboot = 'destroy'
on_crash = 'destroy'

第四步，创建新的非特权域。
# xm create -c rhel6
此命令会启动rhel6的安装界面，其运行于Xen的PV模式。根据提示一步步的安装系统即可。

安装完成后，将上述配置文件中的前两行信息注释，并启用bootloader一行，再使用如前面的xm create命令即可正常启动此域。其修改后的配置文件内容如下：
name = "rhel6"
memory = "512"
disk = [ 'file:/xen/vm1/rhel6.img,xvda,w', ]
vif = [ 'bridge=xenbr0', ]
 
bootloader="/usr/bin/pygrub"
vcpus=1
on_reboot = 'restart'
on_crash = 'destroy







在启动DomU时，可以为其定义可用的虚拟网络接口个数、每个虚拟网络接口的属性等，这仅需要在其对应的配置文件中使用vif选项即可。vif选项的值是一个列表，列表中的每个条目使用单引号引用，用于定义对应虚拟网络接口属性，条目之间使用逗号分隔。比如下面的示例就为当前域定义了三个虚拟网络接口，每个接口的属性均采用了默认配置。
vif = [ ‘ ‘, ‘ ‘, ‘ ‘ ]

每个网络接口可定义的属性语法格式为‘type= TYPE, mac=MAC, bridge=BRIDGE, ip=IPADDR, script= SCRIPT," + \ "backend=DOM, vifname=NAME, rate=RATE, model=MODEL, accel=ACCEL’。
◇	type：网络接口的类型，即其前端设备类型，默认为netfront；其可用的值还有有ioemu，表示使用QEMU的网络驱动；
◇	mac：指定此接口的MAC地址，默认为00:16:3E:(IEEE分配给XenSource的地址段)开头的随机地址；
◇	bridge：指定此接口使用的桥接设备，默认为Dom0内的第一个桥接设备；
◇	ip：为当前域定义固定IP地址，如果网络中存在DHCP服务器，请确保此地址一定没有包含于DHCP的可分配地址范围中；事实上，在DHCP环境中，可以直接在DHCP服务器上为此接口分配固定IP地址，因此，没有必要再使用此参数手动指定；
◇	script：指定用于配置当前接口网络属性的脚本，默认为xend的配置文件中使用vif-script指定的脚本；
◇	vifname：定义当前网络接口的后端设备在Dom0显示的名字；默认为vifDomID.DevID，其在当前域启动时自动生成，并随当前域ID的变化而改变；为其使用易于识别的固定名称有助于后期的管理工作；
◇	rate：为当前接口指定可用带宽，例如rate=10MB/s；
◇	model：由QEMU仿真的网络设备的类型，因此，只有在type的值为ioemu此参数才能意义；其可能的取值有lance、ne2k_isa、ne2k_pci、rt1839和smc91c111。默认为ne2k_pci；

无论DomU中安装的是什么操作系统，为其定义网络接口时指定固定的MAC地址和接口名称通常是很有必要，因为其有助于追踪特定域的报文及当域多次启动后仍能使用相同的网络接口名称从而保证日志信息的连贯性。此外，如果Dom0中定义了多个桥接设备，还应该为桥接的网络接口使用bridge参数指定固定桥接到的桥接设备。下面的示例展示了指定此三个参数的接口定义。
vif = [ ‘vifname=web0.0, mac=00:16:3E:00:00:01, bridge=xenbr0’ ]



save/restore
suspend/resume
pause/unpause






xen实时迁移

基于iscsi来实现。


kernel = "/boot/vmlinuz-3.7.4-1.el6xen.x86_64"
ramdisk = "/boot/initramfs-3.7.4-1.el6xen.x86_64.img"
name = "test"
memory = "128"
disk = [ 'phy:/dev/sdb,xvda,w', ]
vcpus=2
on_reboot = 'restart'
on_crash = 'destroy'
root = "/dev/xvda2 ro"
extra = "selinux=0 init=/sbin/init"


提供配置文件：
	/etc/{passwd,shadow,group,fstab,nsswitch.conf}
提供库文件：
	/lib64/libnss_file.so.*
提供配置文件:/etc/nginx
提供目录：/var/run, /var/log/nginx

网卡驱动：xen-netfront.ko







[xen4]
name=Xen4 Project
baseurl=ftp://172.16.0.1/pub/Sources/6.x86_64/xen4/x86_64/
gpgcheck=0
cost=500





xen实时迁移，xm, 配置文件的使用细节


[et-virt07 ~]# grep xend-relocation /etc/xen/xend-config.sxp |grep -v '#'
(xend-relocation-server yes)
(xend-relocation-port 8002)
(xend-relocation-address '')
(xend-relocation-hosts-allow '')
[et-virt08 ~]# grep xend-relocation /etc/xen/xend-config.sxp |grep -v '#'
(xend-relocation-server yes)
(xend-relocation-port 8002)
(xend-relocation-address '')
(xend-relocation-hosts-allow '')










xl, xm/xend, xe/xapi

xm
	create, destroy, delete, migrate

	pause: 虚拟机暂停于内存中；unpause

xen: 共享存储
	iSCSI, FC, NFS, GlusterFS

KVM: Kernel-based VM


虚拟化模型：
type1: Hosted
type2: bare-metal

虚拟化技术：
	full
	para
	hybrid：pv on hvm

KVM：完全虚拟化
	CPU: HVM (VT-x, AMD-V, EPT, NPT)
	Qemu: 模拟
	virtio: 通用半虚拟化技术
		KVM, virtio
使用virtio的KVM，hybrid





KVM: (64bits), CPU()
	rhel5.9
	rhel6.4

CPU: HVM
	grep -o -E 'svm|vmx' /proc/cpuinfo

KVM：

modprobe
	kvm_intel, kvm_amd
	kvm

modprobe kvm



安装：

KVM: Full
Xen: PV, Full(HVM+qemu), PV on HVM









模式：内核模式、用户模式、来宾模式
VCPU：用线程模拟实现CPU




以色列Qumranet公司



KVM组件

KVM主要两类组件组成：
◇	/dev/kvm：管理虚拟机的设备文件，用户空间的程序可通过其ioctl()系统调用集来完成虚拟机的创建启动等管理工作；它是一个字符设备；其主要完成的操作包括：
	创建虚拟机；
	为虚拟机分配内存；
	读、写VCPU的寄存器；
	向VCPU注入中断；
	运行VCPU；
◇	qemu进程：工作于用户空间的组件，用于仿真PC机的I/O类硬件设备；


qemu: qemu-kvm, qemu-img
libvirt: virt-viewer, virt-manager, virsh, virt-install



kvm --> xen --> openvz, lxc






KVM内存管理

KVM继承了Linux系统管理内存的诸多特性，比如，分配给虚拟使用的内存可以被交换至交换空间、能够使用大内存页以实现更好的性能，以及对NUMA的支持能够让虚拟机高效访问更大的内存空间等。

KVM基于Intel的EPT（Extended Page Table）或AMD的RVI（Rapid Virtualization Indexing）技术可以支持更新的内存虚拟功能，这可以降低CPU的占用率，并提供较好的吞吐量。

此外，KVM还借助于KSM（Kernel Same-page Merging）这个内核特性实现了内存页面共享。KSM通过扫描每个虚拟机的内存查找各虚拟机间相同的内存页，并将这些内存页合并为一个被各相关虚拟机共享的单独页面。在某虚拟机试图修改此页面中的数据时，KSM会重新为其提供一个新的页面副本。实践中，运行于同一台物理主机上的具有相同GuestOS的虚拟机之间出现相同内存页面的概率是很的，比如共享库、内核或其它内存对象等都有可能表现为相同的内存页，因此，KSM技术可以降低内存占用进而提高整体性能。







# grep  -E  --color  "(vmx|svm)"  /proc/cpuinfo



Popek, Glodberg
	1、等价执行
	2、性能良好
	3、安全隔离

	trap, emulate

tagged tlb


VMM:对IO的驱动有三种模式：
	自主VMM：VMM自行提供驱动和控制台；
	混合VMM：借助于OS提供驱动；
		依赖于外部OS实现特权域
		自我提供特权域
	寄宿式VMM：

IO虚拟化模型：
	模拟
	半虚拟化
	透传








libvirt: 工具实现虚拟机管理：
	装系统：virt-manager, virt-install, virsh
2.5.3.2 使用virt-install创建虚拟机并安装GuestOS
virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。

virt-install命令有许多选项，这些选项大体可分为下面几大类，同时对每类中的常用选项也做出简单说明。
◇	一般选项：指定虚拟机的名称、内存大小、VCPU个数及特性等；
	-n NAME, --name=NAME：虚拟机名称，需全局惟一；
	-r MEMORY, --ram=MEMORY：虚拟机内在大小，单位为MB；
	--vcpus=VCPUS[,maxvcpus=MAX][,sockets=#][,cores=#][,threads=#]：VCPU个数及相关配置；
	--cpu=CPU：CPU模式及特性，如coreduo等；可以使用qemu-kvm -cpu ?来获取支持的CPU模式；
◇	安装方法：指定安装方法、GuestOS类型等；
	-c CDROM, --cdrom=CDROM：光盘安装介质；
	-l LOCATION, --location=LOCATION：安装源URL，支持FTP、HTTP及NFS等，如ftp://172.16.0.1/pub；
	--pxe：基于PXE完成安装；
	--livecd: 把光盘当作LiveCD；
	--os-type=DISTRO_TYPE：操作系统类型，如linux、unix或windows等；
	--os-variant=DISTRO_VARIANT：某类型操作系统的变体，如rhel5、fedora8等；
	-x EXTRA, --extra-args=EXTRA：根据--location指定的方式安装GuestOS时，用于传递给内核的额外选项，例如指定kickstart文件的位置，--extra-args "ks=http://172.16.0.1/class.cfg"
	--boot=BOOTOPTS：指定安装过程完成后的配置选项，如指定引导设备次序、使用指定的而非安装的kernel/initrd来引导系统启动等 ；例如：
	--boot  cdrom,hd,network：指定引导次序；
	--boot kernel=KERNEL,initrd=INITRD,kernel_args=”console=/dev/ttyS0”：指定启动系统的内核及initrd文件；
◇	存储配置：指定存储类型、位置及属性等；
	--disk=DISKOPTS：指定存储设备及其属性；格式为--disk /some/storage/path,opt1=val1，opt2=val2等；常用的选项有：
	device：设备类型，如cdrom、disk或floppy等，默认为disk；
	bus：磁盘总结类型，其值可以为ide、scsi、usb、virtio或xen；
	perms：访问权限，如rw、ro或sh（共享的可读写），默认为rw；
	size：新建磁盘映像的大小，单位为GB；
	cache：缓存模型，其值有none、writethrouth（缓存读）及writeback（缓存读写）；
	format：磁盘映像格式，如raw、qcow2、vmdk等；
	sparse：磁盘映像使用稀疏格式，即不立即分配指定大小的空间；
	--nodisks：不使用本地磁盘，在LiveCD模式中常用；
◇	网络配置：指定网络接口的网络类型及接口属性如MAC地址、驱动模式等；
	-w NETWORK, --network=NETWORK,opt1=val1,opt2=val2：将虚拟机连入宿主机的网络中，其中NETWORK可以为：
	bridge=BRIDGE：连接至名为“BRIDEG”的桥设备；
	network=NAME：连接至名为“NAME”的网络；
其它常用的选项还有：
	model：GuestOS中看到的网络设备型号，如e1000、rtl8139或virtio等；
	mac：固定的MAC地址；省略此选项时将使用随机地址，但无论何种方式，对于KVM来说，其前三段必须为52:54:00；
	--nonetworks：虚拟机不使用网络功能；
◇	图形配置：定义虚拟机显示功能相关的配置，如VNC相关配置；
	--graphics TYPE,opt1=val1,opt2=val2：指定图形显示相关的配置，此选项不会配置任何显示硬件（如显卡），而是仅指定虚拟机启动后对其进行访问的接口；
	TYPE：指定显示类型，可以为vnc、sdl、spice或none等，默认为vnc；
	port：TYPE为vnc或spice时其监听的端口；
	listen：TYPE为vnc或spice时所监听的IP地址，默认为127.0.0.1，可以通过修改/etc/libvirt/qemu.conf定义新的默认值；
	password：TYPE为vnc或spice时，为远程访问监听的服务进指定认证密码；
	--noautoconsole：禁止自动连接至虚拟机的控制台；
◇	设备选项：指定文本控制台、声音设备、串行接口、并行接口、显示接口等；
	--serial=CHAROPTS：附加一个串行设备至当前虚拟机，根据设备类型的不同，可以使用不同的选项，格式为“--serial type,opt1=val1,opt2=val2,...”，例如：
	--serial pty：创建伪终端；
	--serial dev,path=HOSTPATH：附加主机设备至此虚拟机；
	--video=VIDEO：指定显卡设备模型，可用取值为cirrus、vga、qxl或vmvga；

◇	虚拟化平台：虚拟化模型（hvm或paravirt）、模拟的CPU平台类型、模拟的主机类型、hypervisor类型（如kvm、xen或qemu等）以及当前虚拟机的UUID等；
	-v, --hvm：当物理机同时支持完全虚拟化和半虚拟化时，指定使用完全虚拟化；
	-p, --paravirt：指定使用半虚拟化；
	--virt-type：使用的hypervisor，如kvm、qemu、xen等；所有可用值可以使用’virsh capabilities’命令获取；
◇	其它：
	--autostart：指定虚拟机是否在物理启动后自动启动；
	--print-xml：如果虚拟机不需要安装过程(--import、--boot)，则显示生成的XML而不是创建此虚拟机；默认情况下，此选项仍会创建磁盘映像；
	--force：禁止命令进入交互式模式，如果有需要回答yes或no选项，则自动回答为yes；
	--dry-run：执行创建虚拟机的整个过程，但不真正创建虚拟机、改变主机上的设备配置信息及将其创建的需求通知给libvirt；
	-d, --debug：显示debug信息；

尽管virt-install命令有着类似上述的众多选项，但实际使用中，其必须提供的选项仅包括--name、--ram、--disk（也可是--nodisks）及安装过程相关的选项。此外，有时还需要使用括--connect=CONNCT选项来指定连接至一个非默认的hypervisor。

下面这个示例创建一个名为rhel5的虚拟机，
其hypervisor为KVM，
内存大小为512MB，
磁盘为8G的映像文件/var/lib/libvirt/images/rhel5.8.img，
通过boot.iso光盘镜像来引导启动安装过程。

# virt-install \
   --connect qemu:///system \
   --virt-type kvm \
   --name rhel5 \
   --ram 512 \
   --disk path=/var/lib/libvirt/images/rhel5.img,size=8 \
   --graphics vnc \
   --cdrom /tmp/boot.iso \
   --os-variant rhel5

下面的示例将创建一个名为rhel6的虚拟机，
其有两个虚拟CPU，
安装方法为FTP，并指定了ks文件的位置，
磁盘映像文件为稀疏格式，
连接至物理主机上的名为brnet0的桥接网络：
# virt-install \
    --connect qemu:///system \
    --virt-type kvm \
    --name rhel6 \
    --ram 1024 \
    --vcpus 2 \
    --network bridge=brnet0 \
    --disk path=/VMs/images/rhel6.img,size=120,sparse \
    --location ftp://172.16.0.1/rhel6/dvd \
    --extra_args "ks=http://172.16.0.1/rhel6.cfg" \
    --os-variant rhel6 \
    --force 

下面的示例将创建一个名为rhel5.8的虚拟机，
磁盘映像文件为稀疏模式的格式为qcow2且总线类型为virtio，
安装过程不启动图形界面（--nographics），
但会启动一个串行终端将安装过程以字符形式显示在当前文本模式下，
虚拟机显卡类型为cirrus：
# virt-install \
--connect qemu:///system \
--virt-type kvm \ 
--name rhel5.8 \ 
--vcpus 2,maxvcpus=4 \
--ram 512 \ 
--disk path=/VMs/images/rhel5.8.img,size=120,format=qcow2,bus=virtio,sparse \ 
--network bridge=brnet0,model=virtio
--nographics \
--location ftp://172.16.0.1/pub \ 
--extra-args "ks=http://172.16.0.1/class.cfg  console=ttyS0  serial" \
--os-variant rhel5 \
--force  \
--video=cirrus

下面的示例则利用已经存在的磁盘映像文件（已经有安装好的系统）创建一个名为rhel5.8的虚拟机：

# virt-install \
    --name rhel5.8
    --ram 512
    --disk /VMs/rhel5.8.img
    --import

每个虚拟机创建后，其配置信息保存在/etc/libvirt/qemu目录中，文件名与虚拟机相同，格式为XML。



virsh uri: 查看当前主机上hypervisor的连接路径；
virsh connect:
virsh define: 创建一个虚拟机，根据事先定义的xml格式的配置文件；创建以后不会自动启动；
virsh create: 创建，创建完成后会自动启动；
virsh undefine: 删除




--boot kernel=KERNEL,initrd=INITRD,kernel_args="console=/dev/ttyS0"



virt-install --connect qemu:///system --ram 128 --name rhel6 --os-type=linux --os-variant=rhel5 --disk path=/kvm/vm1/rhel6.img,device=disk,format=raw --vcpus=2 --vnc --noautoconsole --import

virt-install \
              --name mykernel
              --ram 512
              --disk /home/user/VMs/mydisk.img
              --boot kernel=/tmp/mykernel,initrd=/tmp/myinitrd,kernel_args="console=ttyS0"
              --serial pty






# virt-install \
    --connect qemu:///system \
    --virt-type kvm \
    --name rhel6 \
    --ram 512 \
    --vcpus 2 \
    --network bridge=br0 \
    --disk path=/VMs/images/centos6.img,size=120,sparse \
    --location http://172.16.0.1/cobbler/ks_mirror/centos-6.4-x86_64/ \
    --extra-args “ks=http://172.16.0.1/centos6.x86_64.cfg” \
    --os-variant rhel6 \
    --force 



virsh的几个常用命令：
创建虚拟机：事先准备好xml格式的配置文件，可以dump其它已运行的虚拟机的；
create: 创建并启动；
defince: 仅创建

删除虚拟机：destroy --> undefine --> delete 相关的各文件; 


动态改变CPU和memory：
vcpucount, vcpuinfo
setmem, setvcpu

事先定义好硬盘：qemu-img
attach-disk, detach-disk


显示虚拟相关信息：
cpustats: 需要事先开启cgroup中CPUACCT功能; 
list: 
desc

domdisplay：显示虚拟机的URI；
vncdisplay: 显示虚拟机的vnc连接地址；


virt-install \
--connect qemu:///system \
--virt-type kvm \
--name rhel5.8 \
--vcpus 1,maxvcpus=2 \
--ram 512 \
--disk "path=/VMs/images/rhel5.8.img,size=120,format=qcow2" --network "bridge=br0,model=virtio" \
--nographics \
--location http://172.16.0.1/cobbler/ks_mirror/rhel-5.8-i386/ \
--extra-args "ks=http://172.16.0.1/workstation.cfg  console=ttyS0  serial" \
--os-variant rhel5 \
--force  \
--video=cirrus







2.5.6 使用qemu-kvm管理KVM虚拟机

Qemu是一个广泛使用的开源计算机仿真器和虚拟机。当作为仿真器时，可以在一种架构(如PC机)下运行另一种架构(如ARM)下的操作系统和程序。而通过动态转化，其可以获得很高的运行效率。当作为一个虚拟机时，qemu可以通过直接使用真机的系统资源，让虚拟系统能够获得接近于物理机的性能表现。qemu支持xen或者kvm模式下的虚拟化。当用kvm时，qemu可以虚拟x86、服务器和嵌入式powerpc，以及s390的系统。

QEMU 当运行与主机架构相同的目标架构时可以使用 KVM。例如，当在一个x86兼容处理器上运行 qemu-system-x86 时，可以利用 KVM 加速——为宿主机和客户机提供更好的性能。

Qemu有如下几个部分组成：
◇	处理器模拟器(x86、PowerPC和Sparc)；
◇	仿真设备(显卡、网卡、硬盘、鼠标等)；
◇	用于将仿真设备连接至主机设备(真实设备)的通用设备；
◇	模拟机的描述信息；
◇	调试器；
◇	与模拟器交互的用户接口；

2.5.6.1 使用qemu-kvm安装Guest

如2.5.5中所述，基于libvirt的工具如virt-manager和virt-install提供了非常便捷的虚拟机管理接口，但它们事实上上经二次开发后又封装了qemu-kvm的工具。因此，直接使用qemu-kvm命令也能够完成此前的任务。

2.5.6.1.1 qemu-kvm命令

在RHEL6上，qemu-kvm位于/usr/libexec目录中。由于此目录不属于PATH环境变量，故无法直接使用，这样也阻止了可以直接使用qemu作为创建并管理虚拟机。如若想使用qemu虚拟机，可以通过将/usr/libexec/qemu-kvm链接为/usr/bin/qemu实现。

# ln  -sv  /usr/lib/exec/qemu-kvm  /usr/bin/qemu-kvm

qemu-kvm命令使用格式为“qemu-kvm  [options]  [disk_image]”，其选项非常多，不过，大致可分为如下几类。

◇	标准选项；
◇	USB选项；
◇	显示选项；
◇	i386平台专用选项；
◇	网络选项；
◇	字符设备选项；
◇	蓝牙相关选项；
◇	Linux系统引导专用选项；
◇	调试/专家模式选项；
◇	PowerPC专用选项；
◇	Sparc32专用选项；

考虑到篇幅及使用需要，这里介绍的选项主要涉及到标准选项、显示选项、i386平台专用选项及Linux系统引导专用选项等相关的选项。

2.5.6.1.2 qemu-kvm的标准选项

qemu-kvm的标准选项主要涉及指定主机类型、CPU模式、NUMA、软驱设备、光驱设备及硬件设备等。
◇	-name name：设定虚拟机名称；
◇	-M machine：指定要模拟的主机类型，如Standard PC、ISA-only PC或Intel-Mac等，可以使用“qemu-kvm -M ?”获取所支持的所有类型；
◇	-m megs：设定虚拟机的RAM大小；
◇	-cpu model：设定CPU模型，如coreduo、qemu64等，可以使用“qemu-kvm -cpu ?”获取所支持的所有模型；
◇	-smp n[,cores=cores][,threads=threads][,sockets=sockets][,maxcpus=maxcpus]：设定模拟的SMP架构中CPU的个数等、每个CPU的核心数及CPU的socket数目等；PC机上最多可以模拟255颗CPU；maxcpus用于指定热插入的CPU个数上限；
◇	-numa opts：指定模拟多节点的numa设备；
◇	-fda file
◇	-fdb file：使用指定文件(file)作为软盘镜像，file为/dev/fd0表示使用物理软驱；
◇	-hda file
◇	-hdb file
◇	-hdc file
◇	-hdd file：使用指定file作为硬盘镜像；
◇	-cdrom file：使用指定file作为CD-ROM镜像，需要注意的是-cdrom和-hdc不能同时使用；将file指定为/dev/cdrom可以直接使用物理光驱；
◇	-drive option[,option[,option[,...]]]：定义一个硬盘设备；可用子选项有很多。
	file=/path/to/somefile：硬件映像文件路径；
	if=interface：指定硬盘设备所连接的接口类型，即控制器类型，如ide、scsi、sd、mtd、floppy、pflash及virtio等；
	index=index：设定同一种控制器类型中不同设备的索引号，即标识号；
	media=media：定义介质类型为硬盘(disk)还是光盘(cdrom)；
	snapshot=snapshot：指定当前硬盘设备是否支持快照功能：on或off；
	cache=cache：定义如何使用物理机缓存来访问块数据，其可用值有none、writeback、unsafe和writethrough四个；
	format=format：指定映像文件的格式，具体格式可参见qemu-img命令；
◇	-boot [order=drives][,once=drives][,menu=on|off]：定义启动设备的引导次序，每种设备使用一个字符表示；不同的架构所支持的设备及其表示字符不尽相同，在x86 PC架构上，a、b表示软驱、c表示第一块硬盘，d表示第一个光驱设备，n-p表示网络适配器；默认为硬盘设备；
	-boot order=dc,once=d

2.5.6.1.3 qemu-kvm的显示选项

显示选项用于定义虚拟机启动后的显示接口相关类型及属性等。

◇	-nographic：默认情况下，qemu使用SDL来显示VGA输出；而此选项用于禁止图形接口，此时,qemu类似一个简单的命令行程序，其仿真串口设备将被重定向到控制台；
◇	-curses：禁止图形接口，并使用curses/ncurses作为交互接口；
◇	-alt-grab：使用Ctrl+Alt+Shift组合键释放鼠标；
◇	-ctrl-grab：使用右Ctrl键释放鼠标；
◇	-sdl：启用SDL；
◇	-spice option[,option[,...]]：启用spice远程桌面协议；其有许多子选项，具体请参照qemu-kvm的手册；
◇	-vga type：指定要仿真的VGA接口类型，常见类型有：
	cirrus：Cirrus Logic GD5446显示卡；
	std：带有Bochs VBI扩展的标准VGA显示卡；
	vmware：VMWare SVGA-II兼容的显示适配器；
	qxl：QXL半虚拟化显示卡；与VGA兼容；在Guest中安装qxl驱动后能以很好的方式工作，在使用spice协议时推荐使用此类型；
	none：禁用VGA卡；
◇	-vnc display[,option[,option[,...]]]：默认情况下，qemu使用SDL显示VGA输出；使用-vnc选项，可以让qemu监听在VNC上，并将VGA输出重定向至VNC会话；使用此选项时，必须使用-k选项指定键盘布局类型；其有许多子选项，具体请参照qemu-kvm的手册；

	display:
		（1）host:N
			172.16.100.7:1, 监听于172.16.100.7主的5900+N的端口上
		(2) unix:/path/to/socket_file
		(3) none

	options:
		password: 连接时需要验正密码；设定密码通过monitor接口使用change
		reverse: “反向”连接至某处于监听状态的vncview上；

	-monitor stdio：表示在标准输入输出上显示monitor界面
	-nographic
		Ctrl-a, c: 在console和monitor之间切换
		Ctrl-a, h: 显示帮助信息


SDL: Simple Directmedia Layer
VNC: Virtual Network Computing，基于RFB

monitor:
	help
	info

		cpus

		kvm


	Ha, Hb
		Hb:
			qemu-kvm                  -incoming tcp:0:6767	
			
		Ha: 
			monitor: migrate tcp:Hb:6767	













2.5.6.1.4 i386平台专用选项

◇	-no-acpi：禁用ACPI功能，GuestOS与ACPI出现兼容问题时使用此选项；
◇	-balloon none：禁用balloon设备；
◇	-balloon virtio[,addr=addr]：启用virtio balloon设备；

2.5.6.1.5 网络属性相关选项

网络属性相关选项用于定义网络设备接口类型及其相关的各属性等信息。这里只介绍nic、tap和user三种类型网络接口的属性，其它类型请参照qemu-kvm手册。

◇	-net nic[,vlan=n][,macaddr=mac][,model=type][,name=name][,addr=addr][,vectors=v]：创建一个新的网卡设备并连接至vlan n中；PC架构上默认的NIC为e1000，macaddr用于为其指定MAC地址，name用于指定一个在监控时显示的网上设备名称；emu可以模拟多个类型的网卡设备，如virtio、i82551、i82557b、i82559er、ne2k_isa、pcnet、rtl8139、e1000、smc91c111、lance及mcf_fec等；不过，不同平台架构上，其支持的类型可能只包含前述列表的一部分，可以使用“qemu-kvm -net nic,model=?”来获取当前平台支持的类型；
◇	-net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile]：通过物理机的TAP网络接口连接至vlan n中，使用script=file指定的脚本(默认为/etc/qemu-ifup)来配置当前网络接口，并使用downscript=file指定的脚本(默认为/etc/qemu-ifdown)来撤消接口配置；使用script=no和downscript=no可分别用来禁止执行脚本；
◇	-net user[,option][,option][,...]：在用户模式配置网络栈，其不依赖于管理权限；有效选项有：
	vlan=n：连接至vlan n，默认n=0；
	name=name：指定接口的显示名称，常用于监控模式中；
	net=addr[/mask]：设定GuestOS可见的IP网络，掩码可选，默认为10.0.2.0/8；
	host=addr：指定GuestOS中看到的物理机的IP地址，默认为指定网络中的第二个，即x.x.x.2；
	dhcpstart=addr：指定DHCP服务地址池中16个地址的起始IP，默认为第16个至第31个，即x.x.x.16-x.x.x.31；
	dns=addr：指定GuestOS可见的dns服务器地址；默认为GuestOS网络中的第三个地址，即x.x.x.3；
	tftp=dir：激活内置的tftp服务器，并使用指定的dir作为tftp服务器的默认根目录；
	bootfile=file：BOOTP文件名称，用于实现网络引导GuestOS；如：qemu -hda linux.img -boot n -net user,tftp=/tftpserver/pub,bootfile=/pxelinux.0

brctl addbr br0
brctl addif br0 eth0

brctl addbr br1



2.5.6.1.6 一个使用示例

下面的命令创建了一个名为rhel5.8的虚拟机，其RAM大小为512MB，有两颗CPU的SMP架构，默认引导设备为硬盘，有一个硬盘设备和一个光驱设备，网络接口类型为virtio，VGA模式为cirrus，并启用了balloon功能。

# qemu-kvm -name "rhel5.8" -m 512 \
-smp 2 -boot d \
-drive file=/VM/images/rhel5.8/hda,if=virtio,index=0,media=disk,format=qcow2 \
-drive file=/isos/rhel-5.8.iso,index=1,media=cdrom \
-net nic,model=virtio,macaddr=52:54:00:A5:41:1E \
-vga cirrus -balloon virtio

需要注意的是，上述命令中使用的硬盘映像文件/VM/images/rhel5.8/hda需要事先使用qemu-img命令创建，其具体使用格式请见下节介绍。

在虚拟机创建并安装GuestOS完成之后，可以免去光驱设备直接启动之。命令如下所示。

# qemu-kvm -name "rhel5.8" -m 512 \
-smp 2 -boot d \
-drive file=/VM/images/rhel5.8/hda,if=virtio,index=0,media=disk,format=qcow2 \
-net nic,model=virtio,macaddr=52:54:00:A5:41:1E \
-vga cirrus -balloon virtio

2.5.6.1.7 使用qemu-img管理磁盘映像

qemu-img是qemu用来实现磁盘映像管理的工具组件，其有许多子命令，分别用于实现不同的管理功能，而每一个子命令也都有一系列不同的选项。其使用语法格式为“qemu-img  subcommand  [options]”，支持的子命令如下。

◇	create：创建一个新的磁盘映像文件；
◇	check：检查磁盘映像文件中的错误；
◇	convert：转换磁盘映像的格式；
◇	info：显示指定磁盘映像的信息；
◇	snapshot：管理磁盘映像的快照；
◇	commit：提交磁盘映像的所有改变；
◇	rbase：基于某磁盘映像创建新的映像文件；
◇	resize：增大或缩减磁盘映像文件的大小；

使用create子命令创建磁盘映像的命令格式为“create [-f fmt] [-o options] filename [size]”，例如下面的命令创建了一个格式为qcow2的120G的稀疏磁盘映像文件。

# qemu-img create -f qcow2  /VM/images/rhel5.8/hda 120G
Formatting '/VM/images/rhel5.8/hda', fmt=qcow2 size=128849018880 encryption=off cluster_size=65536

更进一步的使用信息请参照手册页。





tap: 二层虚拟通道 
tun：三层虚拟通道


qemu-kvm的其它的用到的参数：
	动态迁移时用到的选项：-incoming tcp:0:PORT
	让qemu-kvm进程运行于后台：-daemonize
	开启USB总线：-usb 
		GuestOS为Windows时，-usb -usbdevice tablet  用于实现鼠标定位
	打开KVM的支持：qemu-kvm默认就启用了此选项
		-enable-kvm
	打声音设备：-soundhw

	设定iscsi存储设备：
		-iscsi [user=USERNAME][,password=PASSWORD][,initiator-name=iqn]
		通过URL指定使用的iscsi设备
			iscsi://<target_ip>[:port]/<target_iqn>/<lun>

		# qemu-kvm  -iscsi initiator-name=    -drive file=iscsi://tgt.magedu.com/iqn.2014-05.com.magedu.tgt1/1

	指定使用的bios文件：
		-bios /path/to/some_bios_program

	使用外部内核及ramdisk文件：
		-kernel
		-initrd
		-append: 向内核传递的参数列表












